{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"HF_HOME\"] = (\n",
    "#     \"/media/s44504/3b01c699-3670-469b-801f-13880b9cac56/huggingface/\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# import soundfile as sf\n",
    "# from pathlib import Path\n",
    "# import os\n",
    "# import numpy as np\n",
    "\n",
    "# # dataset_list = [\"en-US-Madison\", \"en-AU-Zak\"] # range(0, 5)\n",
    "# dataset_list = [\"en-UK-Thalia\"] # (range(47, 57))\n",
    "# # dataset_list = [\"sw-TZ-Victoria\"]\n",
    "# # dataset_list = [\"id-ID-Althaf\"]\n",
    "\n",
    "# for dataset_uri in dataset_list:\n",
    "#     dataset = load_dataset(\"bookbot/en-bookbot-studio\", num_proc=os.cpu_count)\n",
    "#     # dataset = load_dataset(\"bookbot/sw-TZ-Victoria\", num_proc=os.cpu_count)\n",
    "#     dataset = dataset.filter(lambda example: example[\"speaker\"] == dataset_uri)\n",
    "#     dataset = dataset[\"train\"].select(range(0, 5))\n",
    "#     dataset_name = Path(dataset_uri).stem\n",
    "\n",
    "#     # Specify the directory where you want to save the WAV files\n",
    "#     output_directory = f\"{dataset_name}\"\n",
    "\n",
    "#     # Ensure the output directory exists\n",
    "#     os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "#     # Initialize an empty list to hold the audio data arrays\n",
    "#     concatenated_audio = []\n",
    "\n",
    "#     # Iterate over each item in the dataset\n",
    "#     for i, item in enumerate(dataset):\n",
    "#         # Extract the audio array and sampling rate\n",
    "#         audio_array = item[\"audio\"][\"array\"]\n",
    "#         sampling_rate = item[\"audio\"][\"sampling_rate\"]\n",
    "#         try:\n",
    "#             id = item[\"id\"]\n",
    "#         except:\n",
    "#             id = i\n",
    "\n",
    "#         # Define the output file path\n",
    "#         output_file_path = os.path.join(output_directory, f\"{id}.wav\")\n",
    "\n",
    "#         # Write the audio data to a WAV file\n",
    "#         sf.write(output_file_path, audio_array, sampling_rate)\n",
    "\n",
    "#         # Append the audio data to the list\n",
    "#         concatenated_audio.append(audio_array)\n",
    "\n",
    "#         silence_length_samples = int((350 / 1000) * sampling_rate)\n",
    "#         silence_audio = np.zeros(silence_length_samples)\n",
    "#         concatenated_audio.append(silence_audio)\n",
    "\n",
    "#         print(f\"Written file {output_file_path}\")\n",
    "\n",
    "#     # Concatenate the audio data arrays\n",
    "#     concatenated_audio = np.concatenate(concatenated_audio)\n",
    "\n",
    "#     # Define the output file path for the concatenated audio\n",
    "#     concatenated_file_path = os.path.join(output_directory, \"concatenated_audio.wav\")\n",
    "\n",
    "#     # Write the concatenated audio data to a WAV file\n",
    "#     sf.write(concatenated_file_path, concatenated_audio, sampling_rate)\n",
    "\n",
    "#     print(f\"Concatenated audio written to {concatenated_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated audio written to /home/s44504/StyleTTS2/Demo/id-ID-Althaf/concatenated_audio.wav\n"
     ]
    }
   ],
   "source": [
    "# # Iterate over each item in the dataset\n",
    "# import os\n",
    "# import soundfile as sf\n",
    "# import numpy as np\n",
    "\n",
    "# path_dir = \"/home/s44504/StyleTTS2/Demo/id-ID-Althaf\"\n",
    "\n",
    "# # Initialize the concatenated_audio list\n",
    "# concatenated_audio = []\n",
    "\n",
    "# for i, filename in enumerate(os.listdir(path_dir)):\n",
    "#     if filename.endswith(\".wav\"):\n",
    "#         file_path = os.path.join(path_dir, filename)\n",
    "#         # Load the audio file using soundfile\n",
    "#         audio_array, sampling_rate = sf.read(file_path)\n",
    "\n",
    "#         # Add 350ms of silence\n",
    "#         silence_length_samples = int((400 / 1000) * sampling_rate)\n",
    "#         silence_audio = np.zeros(silence_length_samples)\n",
    "\n",
    "#         # Concatenate the audio with silence\n",
    "#         audio_with_silence = np.concatenate([audio_array, silence_audio])\n",
    "\n",
    "#         # Append the audio data with silence to the list\n",
    "#         concatenated_audio.append(audio_with_silence)\n",
    "\n",
    "# output_directory = \"/home/s44504/StyleTTS2/Demo/id-ID-Althaf\"\n",
    "\n",
    "# # Concatenate all audio arrays in the list\n",
    "# final_concatenated_audio = np.concatenate(concatenated_audio)\n",
    "\n",
    "# # Define the output file path for the concatenated audio\n",
    "# concatenated_file_path = os.path.join(output_directory, \"concatenated_audio.wav\")\n",
    "\n",
    "# # Write the concatenated audio data to a WAV file\n",
    "# sf.write(concatenated_file_path, final_concatenated_audio, sampling_rate)\n",
    "\n",
    "# print(f\"Concatenated audio written to {concatenated_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/s44504/StyleTTS2\n",
      "177\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.cuda.set_device(1)\n",
    "\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "%cd ..\n",
    "\n",
    "# load packages\n",
    "import time\n",
    "import random\n",
    "import yaml\n",
    "from munch import Munch\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import librosa\n",
    "from nltk.tokenize import word_tokenize\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "\n",
    "from models import *\n",
    "from utils import *\n",
    "from text_utils import TextCleaner\n",
    "textcleaner = TextCleaner()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_mel = torchaudio.transforms.MelSpectrogram(\n",
    "    n_mels=80, n_fft=2048, win_length=1200, hop_length=300\n",
    ")\n",
    "mean, std = -4, 4\n",
    "\n",
    "\n",
    "def length_to_mask(lengths):\n",
    "    mask = (\n",
    "        torch.arange(lengths.max())\n",
    "        .unsqueeze(0)\n",
    "        .expand(lengths.shape[0], -1)\n",
    "        .type_as(lengths)\n",
    "    )\n",
    "    mask = torch.gt(mask + 1, lengths.unsqueeze(1))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def preprocess(wave):\n",
    "    wave_tensor = torch.from_numpy(wave).float()\n",
    "    mel_tensor = to_mel(wave_tensor)\n",
    "    mel_tensor = (torch.log(1e-5 + mel_tensor.unsqueeze(0)) - mean) / std\n",
    "    return mel_tensor\n",
    "\n",
    "\n",
    "def compute_style(path):\n",
    "    wave, sr = librosa.load(path, sr=24000)\n",
    "    audio, index = librosa.effects.trim(wave, top_db=30)\n",
    "    if sr != 24000:\n",
    "        audio = librosa.resample(audio, sr, 24000)\n",
    "    mel_tensor = preprocess(audio).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ref_s = model.style_encoder(mel_tensor.unsqueeze(1))\n",
    "        ref_p = model.predictor_encoder(mel_tensor.unsqueeze(1))\n",
    "\n",
    "    return torch.cat([ref_s, ref_p], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s44504/miniconda3/envs/vad/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "/home/s44504/miniconda3/envs/vad/lib/python3.9/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# load phonemizer\n",
    "\n",
    "\n",
    "# config = yaml.safe_load(open(\"Models/EN-Multi-ID-Althaf/config_ft_en_multi_id_althaf.yml\"))\n",
    "config = yaml.safe_load(\n",
    "    open(\n",
    "        \"/home/s44504/StyleTTS2-Demo/Models/EN-Multi-ID-Althaf-emphasis/config_ft_en_multi_id_althaf_sw_victoria.yml\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# load pretrained ASR model\n",
    "ASR_config = config.get(\"ASR_config\", False)\n",
    "ASR_path = config.get(\"ASR_path\", False)\n",
    "text_aligner = load_ASR_models(ASR_path, ASR_config)\n",
    "\n",
    "# load pretrained F0 model\n",
    "F0_path = config.get(\"F0_path\", False)\n",
    "pitch_extractor = load_F0_models(F0_path)\n",
    "\n",
    "# load BERT model\n",
    "from Utils.PLBERT.util import load_plbert\n",
    "\n",
    "BERT_path = config.get(\"PLBERT_dir\", False)\n",
    "plbert = load_plbert(BERT_path)\n",
    "\n",
    "model_params = recursive_munch(config[\"model_params\"])\n",
    "model = build_model(model_params, text_aligner, pitch_extractor, plbert)\n",
    "_ = [model[key].eval() for key in model]\n",
    "_ = [model[key].to(device) for key in model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params_whole = torch.load(\"Models/EN-Multi-ID-Althaf/epoch_2nd_00024.pth\", map_location=\"cpu\")\n",
    "params_whole = torch.load(\n",
    "    \"/home/s44504/StyleTTS2-Demo/Models/EN-Multi-ID-Althaf-emphasis/epoch_2nd_00029.pth\",\n",
    "    map_location=\"cpu\",\n",
    ")\n",
    "# params_whole = torch.load(\"Models/LibriTTS/epochs_2nd_00020.pth\", map_location=\"cpu\")\n",
    "params = params_whole[\"net\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert loaded\n",
      "bert_encoder loaded\n",
      "predictor loaded\n",
      "decoder loaded\n",
      "text_encoder loaded\n",
      "predictor_encoder loaded\n",
      "style_encoder loaded\n",
      "diffusion loaded\n",
      "text_aligner loaded\n",
      "pitch_extractor loaded\n",
      "mpd loaded\n",
      "msd loaded\n",
      "wd loaded\n"
     ]
    }
   ],
   "source": [
    "for key in model:\n",
    "    if key in params:\n",
    "        print(\"%s loaded\" % key)\n",
    "        try:\n",
    "            model[key].load_state_dict(params[key])\n",
    "        except:\n",
    "            from collections import OrderedDict\n",
    "\n",
    "            state_dict = params[key]\n",
    "            new_state_dict = OrderedDict()\n",
    "            for k, v in state_dict.items():\n",
    "                name = k[7:]  # remove `module.`\n",
    "                new_state_dict[name] = v\n",
    "            # load params\n",
    "            model[key].load_state_dict(new_state_dict, strict=False)\n",
    "#             except:\n",
    "#                 _load(params[key], model[key])\n",
    "_ = [model[key].eval() for key in model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Modules.diffusion.sampler import DiffusionSampler, ADPM2Sampler, KarrasSchedule\n",
    "\n",
    "sampler = DiffusionSampler(\n",
    "    model.diffusion.diffusion,\n",
    "    sampler=ADPM2Sampler(),\n",
    "    sigma_schedule=KarrasSchedule(\n",
    "        sigma_min=0.0001, sigma_max=3.0, rho=9.0\n",
    "    ),  # empirical parameters\n",
    "    clamp=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthesize Speech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(\n",
    "    text,\n",
    "    ref_s,\n",
    "    global_phonemizer,\n",
    "    alpha=0.3,\n",
    "    beta=0.7,\n",
    "    diffusion_steps=5,\n",
    "    embedding_scale=1,\n",
    "    phonemes=False,\n",
    "):\n",
    "    text = text.strip()\n",
    "    if phonemes:\n",
    "        ps = text\n",
    "    else:\n",
    "        ps = global_phonemizer.phonemize([text])[0]\n",
    "    print(f\"ps: {ps}\")\n",
    "    # ps = word_tokenize(ps[0])\n",
    "    # ps = \" \".join(ps)\n",
    "    tokens = textcleaner(ps)\n",
    "    tokens.insert(0, 0)\n",
    "    tokens = torch.LongTensor(tokens).to(device).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_lengths = torch.LongTensor([tokens.shape[-1]]).to(device)\n",
    "        text_mask = length_to_mask(input_lengths).to(device)\n",
    "\n",
    "        t_en = model.text_encoder(tokens, input_lengths, text_mask)\n",
    "        bert_dur = model.bert(tokens, attention_mask=(~text_mask).int())\n",
    "        d_en = model.bert_encoder(bert_dur).transpose(-1, -2)\n",
    "\n",
    "        s_pred = sampler(\n",
    "            noise=torch.randn((1, 256)).unsqueeze(1).to(device),\n",
    "            embedding=bert_dur,\n",
    "            embedding_scale=embedding_scale,\n",
    "            features=ref_s,\n",
    "            num_steps=diffusion_steps,\n",
    "        ).squeeze(1)\n",
    "\n",
    "        s = s_pred[:, 128:]\n",
    "        ref = s_pred[:, :128]\n",
    "\n",
    "        ref = alpha * ref + (1 - alpha) * ref_s[:, :128]\n",
    "        s = beta * s + (1 - beta) * ref_s[:, 128:]\n",
    "\n",
    "        d = model.predictor.text_encoder(d_en, s, input_lengths, text_mask)\n",
    "\n",
    "        x, _ = model.predictor.lstm(d)\n",
    "        duration = model.predictor.duration_proj(x)\n",
    "\n",
    "        duration = torch.sigmoid(duration).sum(axis=-1)\n",
    "        pred_dur = torch.round(duration.squeeze()).clamp(min=1)\n",
    "\n",
    "        pred_aln_trg = torch.zeros(input_lengths, int(pred_dur.sum().data))\n",
    "        c_frame = 0\n",
    "        for i in range(pred_aln_trg.size(0)):\n",
    "            pred_aln_trg[i, c_frame : c_frame + int(pred_dur[i].data)] = 1\n",
    "            c_frame += int(pred_dur[i].data)\n",
    "\n",
    "        # encode prosody\n",
    "        en = d.transpose(-1, -2) @ pred_aln_trg.unsqueeze(0).to(device)\n",
    "        if model_params.decoder.type == \"hifigan\":\n",
    "            asr_new = torch.zeros_like(en)\n",
    "            asr_new[:, :, 0] = en[:, :, 0]\n",
    "            asr_new[:, :, 1:] = en[:, :, 0:-1]\n",
    "            en = asr_new\n",
    "\n",
    "        F0_pred, N_pred = model.predictor.F0Ntrain(en, s)\n",
    "\n",
    "        asr = t_en @ pred_aln_trg.unsqueeze(0).to(device)\n",
    "        if model_params.decoder.type == \"hifigan\":\n",
    "            asr_new = torch.zeros_like(asr)\n",
    "            asr_new[:, :, 0] = asr[:, :, 0]\n",
    "            asr_new[:, :, 1:] = asr[:, :, 0:-1]\n",
    "            asr = asr_new\n",
    "\n",
    "        out = model.decoder(asr, F0_pred, N_pred, ref.squeeze().unsqueeze(0))\n",
    "\n",
    "    return (\n",
    "        out.squeeze().cpu().numpy()[..., :-50]\n",
    "    )  # weird pulse at the end of the model, need to be fixed later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"\"\" StyleTTS 2 is a text to speech model that leverages style diffusion and adversarial training with large speech language models to achieve human level text to speech synthesis. \"\"\"\n",
    "text = \"\"\" \"This is with emphasis\", this is without emphasis. \"\"\"\n",
    "# text = \"\"\" StyleTTS 2 adalah model text to speech yang memanfaatkan style diffusion dan pelatihan adversarial dengan model bahasa ucapan besar untuk mencapai sintesis text to speech level manusia. \"\"\"\n",
    "# text = \"\"\" Halo nama saya Budi dari Jakarta dan saya suka sekali membaca buku tentang sejarah Indonesia. \"\"\"\n",
    "# text = \"\"\" Habari jina langu ni Victoria kutoka Tanzania, na napenda kusoma vitabu vya historia. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import soundfile as sf\n",
    "import phonemizer\n",
    "\n",
    "\n",
    "def synthesize_speech(reference_dicts, file_name, language, phonemes=False):\n",
    "    global_phonemizer = phonemizer.backend.EspeakBackend(\n",
    "        language=language, preserve_punctuation=True, with_stress=True\n",
    "    )\n",
    "    start = time.time()\n",
    "    noise = torch.randn(1, 1, 256).to(device)\n",
    "    idx = 0\n",
    "    for text, path in reference_dicts.items():\n",
    "        try:\n",
    "            # Convert string path to Path object for easier manipulation\n",
    "            ref_s = compute_style(path)\n",
    "            path = Path(path)\n",
    "            # Create the output directory based on the reference path\n",
    "            output_directory = (\n",
    "                path.parent / \"synthesized_multilingual_en_id_sw_filtered_emphasis\"\n",
    "            )\n",
    "            os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "            wav = inference(\n",
    "                text,\n",
    "                ref_s,\n",
    "                global_phonemizer,\n",
    "                alpha=0.3,\n",
    "                beta=0.7,\n",
    "                diffusion_steps=10,\n",
    "                embedding_scale=1,\n",
    "                phonemes=phonemes,\n",
    "            )\n",
    "\n",
    "            rtf = (time.time() - start) / (len(wav) / 24000)\n",
    "            print(f\"RTF = {rtf:5f}\")\n",
    "            import IPython.display as ipd\n",
    "\n",
    "            # print(k + \" Synthesized:\")\n",
    "            # display(ipd.Audio(wav, rate=24000, normalize=False))\n",
    "\n",
    "            sf.write(f\"{output_directory}/{file_name}_{idx}.wav\", wav, 24000)\n",
    "            idx += 1\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "        # print(\"Reference:\")\n",
    "        # display(ipd.Audio(path, rate=24000, normalize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### US MADISON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_dicts = {\n",
    "    \"\"\" \"This is with emphasis\", this is without emphasis. \"\"\": \"/home/s44504/StyleTTS2/Demo/en-US-Madison/concatenated_audio.wav\",\n",
    "    \"\"\" That makes sense, \"I agree.\" \"\"\": \"/home/s44504/StyleTTS2/Demo/en-US-Madison/concatenated_audio.wav\",\n",
    "    \"\"\" This is the \"best meal\" I’ve ever had. \"\"\": \"/home/s44504/StyleTTS2/Demo/en-US-Madison/concatenated_audio.wav\",\n",
    "    \"\"\" \"The presentation was nothing short of \"brilliant.\" \"\"\": \"/home/s44504/StyleTTS2/Demo/en-US-Madison/concatenated_audio.wav\",\n",
    "    \"\"\" I am \"completely committed\" to this project. \"\"\": \"/home/s44504/StyleTTS2/Demo/en-US-Madison/concatenated_audio.wav\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps: \"ðɪs ɪz wɪð ˈɛmfəsɪs\", ðɪs ɪz wɪðˌaʊt ˈɛmfəsɪs. \n",
      "RTF = 0.263952\n",
      "ps: ðæt mˌeɪks sˈɛns, \"aɪ ɐɡɹˈiː.\" \n",
      "RTF = 0.449641\n",
      "ps: ðɪs ɪz ðə \"bˈɛst mˈiːl\" aɪv ˈɛvɚ hˌæd. \n",
      "RTF = 0.440371\n",
      "ps: \"ðə pɹˌɛzəntˈeɪʃən wʌz nˈʌθɪŋ ʃˈɔːɹt ʌv \"bɹˈɪliənt.\" \n",
      "RTF = 0.406957\n",
      "ps: aɪˈæm \"kəmplˈiːtli kəmˈɪɾᵻd\" tə ðɪs pɹˈɑːdʒɛkt. \n",
      "RTF = 0.518686\n"
     ]
    }
   ],
   "source": [
    "synthesize_speech(reference_dicts, language=\"en-us\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emphasis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_dicts = {\n",
    "    \"\"\"ðæt \"mˌeɪks\" \"sˈɛns\", aɪ ɐ\"ɡɹˈiː\".\"\"\": \"/home/s44504/StyleTTS2/Demo/en-US-Madison/concatenated_audio.wav\",\n",
    "    \"\"\"aɪ kəm\"plˈiːt\"li ɐ\"ɡɹˈiː\" wɪð juː.\"\"\": \"/home/s44504/StyleTTS2/Demo/en-US-Madison/concatenated_audio.wav\",\n",
    "    \"\"\"ðæts ɐn \"ˈɪn\"tɹɛstɪŋ \"pˈɔɪnt\".\"\"\": \"/home/s44504/StyleTTS2/Demo/en-US-Madison/concatenated_audio.wav\",\n",
    "    \"\"\"aɪ ɐ\"pɹˈiː\"ʃɪˌeɪt jʊɹ \"ˈɪn\"saɪt.\"\"\": \"/home/s44504/StyleTTS2/Demo/en-US-Madison/concatenated_audio.wav\",\n",
    "    \"\"\"aɪ \"sˈiː\" jʊɹ \"pˈɔɪnt\", ɪɾ \"ˈædz\" \"vˈæl\"juː.\"\"\": \"/home/s44504/StyleTTS2/Demo/en-US-Madison/concatenated_audio.wav\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps: ðæt \"mˌeɪks\" \"sˈɛns\", aɪ ɐ\"ɡɹˈiː\".\n",
      "RTF = 0.032364\n",
      "ps: aɪ kəm\"plˈiːt\"li ɐ\"ɡɹˈiː\" wɪð juː.\n",
      "RTF = 0.081484\n",
      "ps: ðæts ɐn \"ˈɪn\"tɹɛstɪŋ \"pˈɔɪnt\".\n",
      "RTF = 0.117580\n",
      "ps: aɪ ɐ\"pɹˈiː\"ʃɪˌeɪt jʊɹ \"ˈɪn\"saɪt.\n",
      "RTF = 0.169745\n",
      "ps: aɪ \"sˈiː\" jʊɹ \"pˈɔɪnt\", ɪɾ \"ˈædz\" \"vˈæl\"juː.\n",
      "RTF = 0.165179\n"
     ]
    }
   ],
   "source": [
    "synthesize_speech(reference_dicts, language=\"en-us\", phonemes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UK THALIA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_dicts = {}\n",
    "\n",
    "dir_path = Path(\"Demo/en-UK-Thalia\")\n",
    "\n",
    "# Iterate through each file in the directory\n",
    "for k, file_path in enumerate(sorted(dir_path.iterdir())):\n",
    "    if file_path.is_file():\n",
    "        reference_dicts[k] = file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'int' object has no attribute 'strip'\n",
      "'int' object has no attribute 'strip'\n",
      "'int' object has no attribute 'strip'\n",
      "'int' object has no attribute 'strip'\n",
      "'int' object has no attribute 'strip'\n",
      "'int' object has no attribute 'strip'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'int' object has no attribute 'strip'\n",
      "'int' object has no attribute 'strip'\n",
      "'int' object has no attribute 'strip'\n",
      "'int' object has no attribute 'strip'\n",
      "'int' object has no attribute 'strip'\n",
      "'int' object has no attribute 'strip'\n",
      "'int' object has no attribute 'strip'\n",
      "'int' object has no attribute 'strip'\n",
      "'int' object has no attribute 'strip'\n",
      "'int' object has no attribute 'strip'\n"
     ]
    }
   ],
   "source": [
    "synthesize_speech(reference_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AU Zak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference_dicts = {}\n",
    "\n",
    "# dir_path = Path(\"Demo/en-AU-Zak\")\n",
    "\n",
    "# # Iterate through each file in the directory\n",
    "# for k, file_path in enumerate(sorted(dir_path.iterdir())):\n",
    "#     if file_path.is_file():\n",
    "#         reference_dicts[k] = file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_dicts = {\n",
    "    \"\"\"ðæt \"mˌeɪks\" \"sˈɛns\", aɪ ɐ\"ɡɹˈiː\".\"\"\": \"/home/s44504/StyleTTS2/Demo/en-AU-Zak/concatenated_audio.wav\",\n",
    "    \"\"\"aɪ kəm\"plˈiːt\"li ɐ\"ɡɹˈiː\" wɪð juː.\"\"\": \"/home/s44504/StyleTTS2/Demo/en-AU-Zak/concatenated_audio.wav\",\n",
    "    \"\"\"ðæts ɐn \"ˈɪn\"tɹɛstɪŋ \"pˈɔɪnt\".\"\"\": \"/home/s44504/StyleTTS2/Demo/en-AU-Zak/concatenated_audio.wav\",\n",
    "    \"\"\"aɪ ɐ\"pɹˈiː\"ʃɪˌeɪt jʊɹ \"ˈɪn\"saɪt.\"\"\": \"/home/s44504/StyleTTS2/Demo/en-AU-Zak/concatenated_audio.wav\",\n",
    "    \"\"\"aɪ \"sˈiː\" jʊɹ \"pˈɔɪnt\", ɪɾ \"ˈædz\" \"vˈæl\"juː.\"\"\": \"/home/s44504/StyleTTS2/Demo/en-AU-Zak/concatenated_audio.wav\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps: ðæt \"mˌeɪks\" \"sˈɛns\", aɪ ɐ\"ɡɹˈiː\".\n",
      "RTF = 0.039056\n",
      "ps: aɪ kəm\"plˈiːt\"li ɐ\"ɡɹˈiː\" wɪð juː.\n",
      "RTF = 0.066234\n",
      "ps: ðæts ɐn \"ˈɪn\"tɹɛstɪŋ \"pˈɔɪnt\".\n",
      "RTF = 0.116893\n",
      "ps: aɪ ɐ\"pɹˈiː\"ʃɪˌeɪt jʊɹ \"ˈɪn\"saɪt.\n",
      "RTF = 0.151151\n",
      "ps: aɪ \"sˈiː\" jʊɹ \"pˈɔɪnt\", ɪɾ \"ˈædz\" \"vˈæl\"juː.\n",
      "RTF = 0.144055\n"
     ]
    }
   ],
   "source": [
    "synthesize_speech(reference_dicts, \"au_zak_emphasis\", \"en-gb\", phonemes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID ALTHAF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_dicts = {}\n",
    "\n",
    "dir_path = Path(\"Demo/id-ID-Althaf\")\n",
    "\n",
    "# Iterate through each file in the directory\n",
    "for k, file_path in enumerate(sorted(dir_path.iterdir())):\n",
    "    if file_path.is_file():\n",
    "        reference_dicts[k] = file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'int' object has no attribute 'strip'\n",
      "'int' object has no attribute 'strip'\n",
      "'int' object has no attribute 'strip'\n",
      "'int' object has no attribute 'strip'\n",
      "'int' object has no attribute 'strip'\n",
      "'int' object has no attribute 'strip'\n"
     ]
    }
   ],
   "source": [
    "synthesize_speech(reference_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SW Victoria\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_dicts = {}\n",
    "\n",
    "dir_path = Path(\"Demo/sw-TZ-Victoria\")\n",
    "\n",
    "# Iterate through each file in the directory\n",
    "for k, file_path in enumerate(sorted(dir_path.iterdir())):\n",
    "    if file_path.is_file():\n",
    "        reference_dicts[k] = file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'int' object has no attribute 'strip'\n",
      "'int' object has no attribute 'strip'\n",
      "'int' object has no attribute 'strip'\n",
      "'int' object has no attribute 'strip'\n",
      "'int' object has no attribute 'strip'\n",
      "'int' object has no attribute 'strip'\n"
     ]
    }
   ],
   "source": [
    "synthesize_speech(reference_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S2S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def s2s(\n",
    "    text,\n",
    "    ref_s,\n",
    "    target_s,\n",
    "    language,\n",
    "    alpha=0.8,\n",
    "    beta=0.1,\n",
    "    diffusion_steps=10,\n",
    "    embedding_scale=1,\n",
    "    phonemes=False,\n",
    "):\n",
    "    global_phonemizer = phonemizer.backend.EspeakBackend(\n",
    "        language=language, preserve_punctuation=True, with_stress=True\n",
    "    )\n",
    "\n",
    "    text = text.strip()\n",
    "    if phonemes:\n",
    "        ps = text\n",
    "    else:\n",
    "        ps = global_phonemizer.phonemize([text])[0]\n",
    "    print(f\"ps: {ps}\")\n",
    "    # ps = word_tokenize(ps[0])\n",
    "    # ps = \" \".join(ps)\n",
    "    tokens = textcleaner(ps)\n",
    "    tokens.insert(0, 0)\n",
    "    tokens = torch.LongTensor(tokens).to(device).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_lengths = torch.LongTensor([tokens.shape[-1]]).to(device)\n",
    "        text_mask = length_to_mask(input_lengths).to(device)\n",
    "\n",
    "        t_en = model.text_encoder(tokens, input_lengths, text_mask)\n",
    "        bert_dur = model.bert(tokens, attention_mask=(~text_mask).int())\n",
    "        d_en = model.bert_encoder(bert_dur).transpose(-1, -2)\n",
    "\n",
    "        s_pred = sampler(\n",
    "            noise=torch.randn((1, 256)).unsqueeze(1).to(device),\n",
    "            embedding=bert_dur,\n",
    "            embedding_scale=embedding_scale,\n",
    "            features=target_s,  # reference from the same speaker as the embedding\n",
    "            num_steps=diffusion_steps,\n",
    "        ).squeeze(1)\n",
    "\n",
    "        ref = s_pred[:, :128]\n",
    "        s = s_pred[:, 128:]\n",
    "\n",
    "        ref = alpha * ref + (1 - alpha) * ref_s[:, :128]\n",
    "        s = beta * s + (1 - beta) * ref_s[:, 128:]\n",
    "\n",
    "        d = model.predictor.text_encoder(d_en, s, input_lengths, text_mask)\n",
    "\n",
    "        x, _ = model.predictor.lstm(d)\n",
    "        duration = model.predictor.duration_proj(x)\n",
    "\n",
    "        duration = torch.sigmoid(duration).sum(axis=-1)\n",
    "        pred_dur = torch.round(duration.squeeze()).clamp(min=1)\n",
    "\n",
    "        pred_aln_trg = torch.zeros(input_lengths, int(pred_dur.sum().data))\n",
    "        c_frame = 0\n",
    "        for i in range(pred_aln_trg.size(0)):\n",
    "            pred_aln_trg[i, c_frame : c_frame + int(pred_dur[i].data)] = 1\n",
    "            c_frame += int(pred_dur[i].data)\n",
    "\n",
    "        # encode prosody\n",
    "        en = d.transpose(-1, -2) @ pred_aln_trg.unsqueeze(0).to(device)\n",
    "        if model_params.decoder.type == \"hifigan\":\n",
    "            asr_new = torch.zeros_like(en)\n",
    "            asr_new[:, :, 0] = en[:, :, 0]\n",
    "            asr_new[:, :, 1:] = en[:, :, 0:-1]\n",
    "            en = asr_new\n",
    "\n",
    "        F0_pred, N_pred = model.predictor.F0Ntrain(en, s)\n",
    "\n",
    "        asr = t_en @ pred_aln_trg.unsqueeze(0).to(device)\n",
    "        if model_params.decoder.type == \"hifigan\":\n",
    "            asr_new = torch.zeros_like(asr)\n",
    "            asr_new[:, :, 0] = asr[:, :, 0]\n",
    "            asr_new[:, :, 1:] = asr[:, :, 0:-1]\n",
    "            asr = asr_new\n",
    "\n",
    "        out = model.decoder(asr, F0_pred, N_pred, ref.squeeze().unsqueeze(0))\n",
    "\n",
    "    return out.squeeze().cpu().numpy()[..., :-50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps: ðæt \"mˌeɪks\" \"sˈɛns\", aɪ ɐ\"ɡɹˈiː\".\n",
      "ps: aɪ kəm\"plˈiːt\"li ɐ\"ɡɹˈiː\" wɪð juː.\n",
      "ps: ðæts ɐn \"ˈɪn\"tɹɛstɪŋ \"pˈɔɪnt\".\n",
      "ps: aɪ ɐ\"pɹˈiː\"ʃɪˌeɪt jʊɹ \"ˈɪn\"saɪt.\n",
      "ps: aɪ \"sˈiː\" jʊɹ \"pˈɔɪnt\", ɪɾ \"ˈædz\" \"vˈæl\"juː.\n"
     ]
    }
   ],
   "source": [
    "reference_dicts = {\n",
    "    \"\"\"ðæt \"mˌeɪks\" \"sˈɛns\", aɪ ɐ\"ɡɹˈiː\".\"\"\": \"/home/s44504/StyleTTS2/Demo/en-AU-Zak/synthesized_multilingual_en_id_sw_filtered_emphasis/au_zak_emphasis_0.wav\",\n",
    "    \"\"\"aɪ kəm\"plˈiːt\"li ɐ\"ɡɹˈiː\" wɪð juː.\"\"\": \"/home/s44504/StyleTTS2/Demo/en-AU-Zak/synthesized_multilingual_en_id_sw_filtered_emphasis/au_zak_emphasis_1.wav\",\n",
    "    \"\"\"ðæts ɐn \"ˈɪn\"tɹɛstɪŋ \"pˈɔɪnt\".\"\"\": \"/home/s44504/StyleTTS2/Demo/en-AU-Zak/synthesized_multilingual_en_id_sw_filtered_emphasis/au_zak_emphasis_2.wav\",\n",
    "    \"\"\"aɪ ɐ\"pɹˈiː\"ʃɪˌeɪt jʊɹ \"ˈɪn\"saɪt.\"\"\": \"/home/s44504/StyleTTS2/Demo/en-AU-Zak/synthesized_multilingual_en_id_sw_filtered_emphasis/au_zak_emphasis_3.wav\",\n",
    "    \"\"\"aɪ \"sˈiː\" jʊɹ \"pˈɔɪnt\", ɪɾ \"ˈædz\" \"vˈæl\"juː.\"\"\": \"/home/s44504/StyleTTS2/Demo/en-AU-Zak/synthesized_multilingual_en_id_sw_filtered_emphasis/au_zak_emphasis_4.wav\",\n",
    "}\n",
    "ref_althaf = compute_style(\n",
    "    \"/home/s44504/StyleTTS2/Demo/id-ID-Althaf/concatenated_audio.wav\"\n",
    ")\n",
    "\n",
    "\n",
    "idx = 0\n",
    "for text, ref_path in reference_dicts.items():\n",
    "    ref_path = compute_style(ref_path)\n",
    "    wav = s2s(\n",
    "        text,\n",
    "        ref_althaf,\n",
    "        ref_path,\n",
    "        \"en-gb\",\n",
    "        alpha=0.9,\n",
    "        beta=0.1,\n",
    "        diffusion_steps=10,\n",
    "        phonemes=True,\n",
    "    )\n",
    "    target_dir = Path(\"Demo/en-AU-Zak/Zak2Althaf\")\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    sf.write(f\"{target_dir}/althaf-au_{idx}.wav\", wav, 24000)\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globe = Path(\"/root/GLOBE_AU/\")\n",
    "globe_audios = sorted(globe.rglob(\"*.wav\"))\n",
    "\n",
    "ref_zak = compute_style(\"/root/StyleTTS2/Demo/en-AU-Zak/concatenated_audio.wav\")\n",
    "\n",
    "for audio in globe_audios[:5]:\n",
    "    transcript_file = audio.with_suffix(\".txt\")\n",
    "    with open(transcript_file) as f:\n",
    "        transcript = f.read()\n",
    "\n",
    "    ref_globe = compute_style(str(audio))\n",
    "    wav = s2s(transcript, ref_globe, ref_zak, alpha=0.8, beta=0.1, diffusion_steps=10)\n",
    "    sf.write(f\"Demo/en-AU-GLOBE2Zak/en-AU-GLOBE2Zak_{audio.stem}.wav\", wav, 24000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
