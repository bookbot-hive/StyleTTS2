{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/s44504/3b01c699-3670-469b-801f-13880b9cac56/StyleTTS2\n",
      "177\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "%cd ..\n",
    "\n",
    "# load packages\n",
    "import time\n",
    "import random\n",
    "import yaml\n",
    "from munch import Munch\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import librosa\n",
    "from nltk.tokenize import word_tokenize\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "\n",
    "from models import *\n",
    "from utils import *\n",
    "from text_utils import TextCleaner\n",
    "textcleaner = TextCleaner()\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "SAMPLING_RATE = 24000\n",
    "\n",
    "to_mel = torchaudio.transforms.MelSpectrogram(\n",
    "    n_mels=80, n_fft=2048, win_length=1200, hop_length=300\n",
    ")\n",
    "mean, std = -4, 4\n",
    "\n",
    "\n",
    "def length_to_mask(lengths):\n",
    "    mask = (\n",
    "        torch.arange(lengths.max())\n",
    "        .unsqueeze(0)\n",
    "        .expand(lengths.shape[0], -1)\n",
    "        .type_as(lengths)\n",
    "    )\n",
    "    mask = torch.gt(mask + 1, lengths.unsqueeze(1))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def preprocess(wave):\n",
    "    wave_tensor = torch.from_numpy(wave).float()\n",
    "    mel_tensor = to_mel(wave_tensor)\n",
    "    mel_tensor = (torch.log(1e-5 + mel_tensor.unsqueeze(0)) - mean) / std\n",
    "    return mel_tensor\n",
    "\n",
    "\n",
    "def compute_style(path):\n",
    "    wave, sr = librosa.load(path, sr=SAMPLING_RATE)\n",
    "    audio, index = librosa.effects.trim(wave, top_db=30)\n",
    "    if sr != SAMPLING_RATE:\n",
    "        audio = librosa.resample(audio, sr, SAMPLING_RATE)\n",
    "    mel_tensor = preprocess(audio).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ref_s = model.style_encoder(mel_tensor.unsqueeze(1))\n",
    "        ref_p = model.predictor_encoder(mel_tensor.unsqueeze(1))\n",
    "\n",
    "    return torch.cat([ref_s, ref_p], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_list = {\n",
    "    \"en\": {\"config\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/StyleTTS2/Models/EN-Multi-ID-Althaf-emphasis/config_ft_en_multi_id_althaf_sw_victoria.yml\",\n",
    "           \"checkpoint\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/StyleTTS2/Models/EN-Multi-ID-Althaf-emphasis/epoch_2nd_00029.pth\"},\n",
    "    \"sw_multi\": {\"config\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/StyleTTS2/Models/EN-Multi-ID-Althaf-SW-Victoria/config_ft_en_multi_id_althaf_sw_victoria.yml\",\n",
    "           \"checkpoint\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/StyleTTS2/Models/EN-Multi-ID-Althaf-SW-Victoria/epoch_2nd_00019.pth\"},\n",
    "    \"sw_victoria\": {\"config\": \"./Models/SW-Bible-Victoria-20-epochs/config_ft_sw_bible_victoria.yml\",\n",
    "           \"checkpoint\": \"./Models/SW-Bible-Victoria-20-epochs/epoch_2nd_00009.pth\"},\n",
    "    \"sw_althaf\": {\"config\": \"./Models/SW-Bible-Althaf-20-epochs/config_ft_sw_bible_althaf.yml\",\n",
    "           \"checkpoint\": \"./Models/SW-Bible-Althaf-20-epochs/epoch_2nd_00019.pth\"},\n",
    "    \"lj\": {\"config\": \"./Models/LJSpeech/config.yml\",\n",
    "           \"checkpoint\": \"./Models/LJSpeech/epoch_2nd_00100.pth\"}\n",
    "}\n",
    "\n",
    "name = \"sw_multi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utils/ASR/config.yml\n",
      "Utils/ASR/epoch_00080.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s44504/miniconda3/envs/vad/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "/home/s44504/miniconda3/envs/vad/lib/python3.9/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert loaded\n",
      "bert_encoder loaded\n",
      "predictor loaded\n",
      "decoder loaded\n",
      "text_encoder loaded\n",
      "predictor_encoder loaded\n",
      "style_encoder loaded\n",
      "diffusion loaded\n",
      "text_aligner loaded\n",
      "pitch_extractor loaded\n",
      "mpd loaded\n",
      "msd loaded\n",
      "wd loaded\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "config = yaml.safe_load(open(\n",
    "    config_list[name][\"config\"]\n",
    "    ))\n",
    "\n",
    "# load pretrained ASR model\n",
    "ASR_config = config.get(\"ASR_config\", False)\n",
    "ASR_path = config.get(\"ASR_path\", False)\n",
    "print(ASR_config)\n",
    "print(ASR_path)\n",
    "text_aligner = load_ASR_models(ASR_path, ASR_config)\n",
    "\n",
    "# load pretrained F0 model\n",
    "F0_path = config.get(\"F0_path\", False)\n",
    "pitch_extractor = load_F0_models(F0_path)\n",
    "\n",
    "# load BERT model\n",
    "from Utils.PLBERT.util import load_plbert\n",
    "\n",
    "BERT_path = config.get(\"PLBERT_dir\", False)\n",
    "plbert = load_plbert(BERT_path)\n",
    "\n",
    "model_params = recursive_munch(config[\"model_params\"])\n",
    "model = build_model(model_params, text_aligner, pitch_extractor, plbert)\n",
    "_ = [model[key].eval() for key in model]\n",
    "_ = [model[key].to(device) for key in model]\n",
    "\n",
    "params_whole = torch.load(config_list[name][\"checkpoint\"], map_location=\"cpu\")\n",
    "params = params_whole[\"net\"]\n",
    "\n",
    "for key in model:\n",
    "    if key in params:\n",
    "        print(\"%s loaded\" % key)\n",
    "        try:\n",
    "            model[key].load_state_dict(params[key])\n",
    "        except:\n",
    "            from collections import OrderedDict\n",
    "\n",
    "            state_dict = params[key]\n",
    "            \n",
    "            new_state_dict = OrderedDict()\n",
    "            for k, v in state_dict.items():\n",
    "                name = k[7:]  # remove `module.`\n",
    "                new_state_dict[name] = v\n",
    "            # load params\n",
    "            model[key].load_state_dict(new_state_dict, strict=False)\n",
    "#             except:\n",
    "#                 _load(params[key], model[key])\n",
    "_ = [model[key].eval() for key in model]\n",
    "\n",
    "\n",
    "from Modules.diffusion.sampler import DiffusionSampler, ADPM2Sampler, KarrasSchedule\n",
    "\n",
    "sampler = DiffusionSampler(\n",
    "    model.diffusion.diffusion,\n",
    "    sampler=ADPM2Sampler(),\n",
    "    sigma_schedule=KarrasSchedule(\n",
    "        sigma_min=0.0001, sigma_max=3.0, rho=9.0\n",
    "    ),  # empirical parameters\n",
    "    clamp=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthesize Speech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import soundfile as sf\n",
    "import phonemizer\n",
    "\n",
    "\n",
    "def inference(\n",
    "    text,\n",
    "    ref_s,\n",
    "    global_phonemizer,\n",
    "    alpha=0.3,\n",
    "    beta=0.7,\n",
    "    diffusion_steps=5,\n",
    "    embedding_scale=1,\n",
    "    phonemes=False,\n",
    "):\n",
    "    text = text.strip()\n",
    "    if phonemes:\n",
    "        ps = text\n",
    "    else:\n",
    "        ps = global_phonemizer.phonemize([text])[0]\n",
    "    print(f\"ps: {ps}\")\n",
    "    # ps = word_tokenize(ps[0])\n",
    "    # ps = \" \".join(ps)\n",
    "    tokens = textcleaner(ps)\n",
    "    tokens.insert(0, 0)\n",
    "    tokens = torch.LongTensor(tokens).to(device).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_lengths = torch.LongTensor([tokens.shape[-1]]).to(device)\n",
    "        text_mask = length_to_mask(input_lengths).to(device)\n",
    "\n",
    "        t_en = model.text_encoder(tokens, input_lengths, text_mask)\n",
    "        bert_dur = model.bert(tokens, attention_mask=(~text_mask).int())\n",
    "        d_en = model.bert_encoder(bert_dur).transpose(-1, -2)\n",
    "\n",
    "        s_pred = sampler(\n",
    "            noise=torch.randn((1, 256)).unsqueeze(1).to(device),\n",
    "            embedding=bert_dur,\n",
    "            embedding_scale=embedding_scale,\n",
    "            features=ref_s,\n",
    "            num_steps=diffusion_steps,\n",
    "        ).squeeze(1)\n",
    "\n",
    "        s = s_pred[:, 128:]\n",
    "        ref = s_pred[:, :128]\n",
    "\n",
    "        ref = alpha * ref + (1 - alpha) * ref_s[:, :128]\n",
    "        s = beta * s + (1 - beta) * ref_s[:, 128:]\n",
    "\n",
    "        d = model.predictor.text_encoder(d_en, s, input_lengths, text_mask)\n",
    "\n",
    "        x, _ = model.predictor.lstm(d)\n",
    "        duration = model.predictor.duration_proj(x)\n",
    "\n",
    "        duration = torch.sigmoid(duration).sum(axis=-1)\n",
    "        pred_dur = torch.round(duration.squeeze()).clamp(min=1)\n",
    "\n",
    "        pred_aln_trg = torch.zeros(input_lengths, int(pred_dur.sum().data))\n",
    "        c_frame = 0\n",
    "        for i in range(pred_aln_trg.size(0)):\n",
    "            pred_aln_trg[i, c_frame : c_frame + int(pred_dur[i].data)] = 1\n",
    "            c_frame += int(pred_dur[i].data)\n",
    "\n",
    "        # encode prosody\n",
    "        en = d.transpose(-1, -2) @ pred_aln_trg.unsqueeze(0).to(device)\n",
    "        if model_params.decoder.type == \"hifigan\":\n",
    "            asr_new = torch.zeros_like(en)\n",
    "            asr_new[:, :, 0] = en[:, :, 0]\n",
    "            asr_new[:, :, 1:] = en[:, :, 0:-1]\n",
    "            en = asr_new\n",
    "\n",
    "        F0_pred, N_pred = model.predictor.F0Ntrain(en, s)\n",
    "\n",
    "        asr = t_en @ pred_aln_trg.unsqueeze(0).to(device)\n",
    "        if model_params.decoder.type == \"hifigan\":\n",
    "            asr_new = torch.zeros_like(asr)\n",
    "            asr_new[:, :, 0] = asr[:, :, 0]\n",
    "            asr_new[:, :, 1:] = asr[:, :, 0:-1]\n",
    "            asr = asr_new\n",
    "\n",
    "        out = model.decoder(asr, F0_pred, N_pred, ref.squeeze().unsqueeze(0))\n",
    "\n",
    "    return (\n",
    "        out.squeeze().cpu().numpy()[..., :-50]\n",
    "    )  # weird pulse at the end of the model, need to be fixed later\n",
    "\n",
    "def synthesize_speech(reference_dicts, output_directory, file_name, language, phonemes=False, alpha=0.3, beta=0.7, diffusion_steps = 10, embedding_scale=1):\n",
    "    global_phonemizer = phonemizer.backend.EspeakBackend(\n",
    "        language=language, preserve_punctuation=True, with_stress=True\n",
    "    )\n",
    "    start = time.time()\n",
    "    noise = torch.randn(1, 1, 256).to(device)\n",
    "    idx = 0\n",
    "    for text, path in reference_dicts.items():\n",
    "        try:\n",
    "            # Convert string path to Path object for easier manipulation\n",
    "            ref_s = compute_style(path)\n",
    "            path = Path(path)\n",
    "            # Create the output directory based on the reference path\n",
    "            os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "            wav = inference(\n",
    "                text,\n",
    "                ref_s,\n",
    "                global_phonemizer,\n",
    "                alpha=alpha,\n",
    "                beta=beta,\n",
    "                diffusion_steps=diffusion_steps,\n",
    "                embedding_scale=embedding_scale,\n",
    "                phonemes=phonemes,\n",
    "            )\n",
    "\n",
    "            rtf = (time.time() - start) / (len(wav) / SAMPLING_RATE)\n",
    "            print(f\"RTF = {rtf:5f}\")\n",
    "            import IPython.display as ipd\n",
    "\n",
    "            # print(k + \" Synthesized:\")\n",
    "            # display(ipd.Audio(wav, rate=SAMPLING_RATE, normalize=False))\n",
    "\n",
    "            sf.write(f\"{output_directory}/{text}.wav\", wav, SAMPLING_RATE)\n",
    "            print(f\"Audio saved to: {output_directory}/{text}.wav\")\n",
    "            idx += 1\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### US MADISON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_dicts = {\n",
    "    \"\"\"Learning is a journey filled with curiosity, wonder, and discovery. Every child deserves the opportunity to explore and grow at their own pace. By fostering a love for reading and numbers, we empower them to reach their fullest potential.\"\"\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/StyleTTS2/Demo/en-US-Madison/concatenated_audio.wav\",\n",
    "}\n",
    "synthesize_speech(reference_dicts, \"./Demo/en-US-Madison/madison\", \"madison_reference\", language=\"en-us\", alpha=0.3, beta=0.7, embedding_scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXCLAMATION\n",
    "reference_dicts = {\n",
    "    \"\"\"This spicy dish packs a punch!\"\"\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/madison-althaf-exclamations/wavs/line_0001.wav\",\n",
    "\n",
    "}\n",
    "\n",
    "synthesize_speech(reference_dicts, \"./Demo/en-US-Madison/madison2althaf/exclamation/synthesized\", \"exclamation\", language=\"en-us\", alpha=0.5, beta=0.0, embedding_scale=1)\n",
    "\n",
    "### QUESTION\n",
    "# reference = \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/StyleTTS2/Demo/en-US-Madison/madison2althaf/madison_althaf_eleven_labs_reference.mp3\"\n",
    "# reference_dicts = {\n",
    "#     \"\"\"Did anyone remember to check the schedule?\"\"\": reference,\n",
    "#     \"\"\"Could this really be the best solution?\"\"\": reference,\n",
    "#     \"\"\"Who can I reach out to if I have further questions?\"\"\": reference,\n",
    "#     \"\"\"How did you come up with that idea?\"\"\": reference,\n",
    "#     \"\"\"Should we double-check the details before finalising?\"\"\": reference,\n",
    "# }\n",
    "\n",
    "# synthesize_speech(reference_dicts, \"./Demo/en-US-Madison/madison2althaf/question/synthesized\", \"question\", language=\"en-us\", alpha=0.7, beta=1.0, embedding_scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UK THALIA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_dicts = {\n",
    "    \"\"\" StyleTTS 2 is a text to speech model that leverages style diffusion and adversarial training with large speech language models to achieve human level text to speech synthesis. \"\"\": \"/home/s44504/StyleTTS2/Demo/en-UK-Thalia/concatenated_audio.wav\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference = \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/StyleTTS2/Demo/en-UK-Thalia/thalia2althaf/exclamation/eleven_thalia_exclamation_concat.wav\"\n",
    "reference_dicts = {\n",
    "    \"The holiday decorations are stunningly festive!\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/thalia-althaf-exclamations/wavs/line_0001.wav\",\n",
    "}\n",
    "\n",
    "synthesize_speech(reference_dicts, \"./Demo/en-UK-Thalia/thalia2althaf/exclamation/synthesized\", \"exclamation\", language=\"en-gb\", alpha=0.3, beta=0.0, embedding_scale=1)\n",
    "\n",
    "# reference = \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/StyleTTS2/Demo/en-UK-Thalia/thalia2althaf/thalia_althaf_eleven_labs.mp3\"\n",
    "# reference_dicts = {\n",
    "#     \"\"\"Did anyone remember to check the schedule?\"\"\": reference,\n",
    "#     \"\"\"Could this really be the best solution?\"\"\": reference,\n",
    "#     \"\"\"Who can I reach out to if I have further questions?\"\"\": reference,\n",
    "#     \"\"\"How did you come up with that idea?\"\"\": reference,\n",
    "#     \"\"\"Should we double-check the details before finalising?\"\"\": reference,\n",
    "# }\n",
    "\n",
    "# synthesize_speech(reference_dicts, \"./Demo/en-UK-Thalia/thalia2althaf/question/synthesized\", \"question\", language=\"en-gb\", alpha=0.7, beta=0.9, embedding_scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AU Zak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps: ɪt sˈɛd nˈəʊ. \n",
      "RTF = 0.043451\n",
      "Audio saved to: ./Demo/en-AU-Zak/zak2althaf-short-sentences/synthesized/It said no..wav\n",
      "ps: ɐ wˈʊlf kˈʌmz. \n",
      "RTF = 0.069540\n",
      "Audio saved to: ./Demo/en-AU-Zak/zak2althaf-short-sentences/synthesized/A wolf comes..wav\n",
      "ps: ɐn ʌmbɹˈɛlə? \n",
      "RTF = 0.126999\n",
      "Audio saved to: ./Demo/en-AU-Zak/zak2althaf-short-sentences/synthesized/An umbrella?.wav\n",
      "Calculated padded input size per channel: (5 x 4). Kernel size: (5 x 5). Kernel size can't be greater than actual input size\n",
      "Calculated padded input size per channel: (5 x 4). Kernel size: (5 x 5). Kernel size can't be greater than actual input size\n",
      "ps: ðə kˈat ɹˈan. \n",
      "RTF = 0.174621\n",
      "Audio saved to: ./Demo/en-AU-Zak/zak2althaf-short-sentences/synthesized/The cat ran..wav\n",
      "ps: dˈaɹən sˈɛd. \n",
      "RTF = 0.206162\n",
      "Audio saved to: ./Demo/en-AU-Zak/zak2althaf-short-sentences/synthesized/Darren said..wav\n",
      "ps: mˈʌm ɪz ɡlˈad. \n",
      "RTF = 0.246738\n",
      "Audio saved to: ./Demo/en-AU-Zak/zak2althaf-short-sentences/synthesized/Mum is glad..wav\n",
      "Calculated padded input size per channel: (5 x 4). Kernel size: (5 x 5). Kernel size can't be greater than actual input size\n",
      "ps: ðə mˈan nˈɒdɪd. \n",
      "RTF = 0.285630\n",
      "Audio saved to: ./Demo/en-AU-Zak/zak2althaf-short-sentences/synthesized/The man nodded..wav\n",
      "ps: zˈɒmbi ɐtˈak. \n",
      "RTF = 0.314056\n",
      "Audio saved to: ./Demo/en-AU-Zak/zak2althaf-short-sentences/synthesized/Zombie Attack..wav\n",
      "ps: ɹiːˈɑːm fˈast! \n",
      "RTF = 0.366398\n",
      "Audio saved to: ./Demo/en-AU-Zak/zak2althaf-short-sentences/synthesized/Rearm fast!.wav\n",
      "ps: ɐ spˈɛʃəl flˈaʊə. \n",
      "RTF = 0.328319\n",
      "Audio saved to: ./Demo/en-AU-Zak/zak2althaf-short-sentences/synthesized/A Special Flower..wav\n",
      "Calculated padded input size per channel: (5 x 4). Kernel size: (5 x 5). Kernel size can't be greater than actual input size\n",
      "ps: ɐ fˈɑːm dˈɒɡ. \n",
      "RTF = 0.441636\n",
      "Audio saved to: ./Demo/en-AU-Zak/zak2althaf-short-sentences/synthesized/A farm dog..wav\n",
      "ps: ɪt lˈandz ˈɒn. \n",
      "RTF = 0.488749\n",
      "Audio saved to: ./Demo/en-AU-Zak/zak2althaf-short-sentences/synthesized/It lands on..wav\n",
      "ps: ɪt hˈɪt mˌiː. \n",
      "RTF = 0.619305\n",
      "Audio saved to: ./Demo/en-AU-Zak/zak2althaf-short-sentences/synthesized/It hit me..wav\n",
      "Calculated padded input size per channel: (5 x 4). Kernel size: (5 x 5). Kernel size can't be greater than actual input size\n",
      "ps: tˈɪk ɪz sˈeɪf! \n",
      "RTF = 0.632935\n",
      "Audio saved to: ./Demo/en-AU-Zak/zak2althaf-short-sentences/synthesized/Tik is safe!.wav\n",
      "Calculated padded input size per channel: (5 x 4). Kernel size: (5 x 5). Kernel size can't be greater than actual input size\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "### Sentence\n",
    "# reference = \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/StyleTTS2/Demo/en-AU-Zak/eleven_labs_zak2althaf_reference.mp3\"\n",
    "# reference_dicts = {\n",
    "#     \"\"\"Hi there!\"\"\": reference,\n",
    "#     \"\"\"Oh no!\"\"\": reference,\n",
    "# }\n",
    "\n",
    "# synthesize_speech(reference_dicts, \"./Demo/en-AU-Zak/zak\", \"zak\", language=\"en-gb\", alpha=0.7, beta=0.9, embedding_scale=1)\n",
    "\n",
    " \n",
    "# # EXCLAMATION\n",
    "# reference = \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/StyleTTS2/Demo/en-AU-Zak/eleven_labs_zak2althaf_reference.mp3\"\n",
    "# reference_dicts = {\n",
    "#     \"\"\"Congratulation on your achievement!\"\"\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/StyleTTS2/Demo/en-AU-Zak/zak2althaf-exclamation/eleven-labs/Congratulations on your achievement!!!!.mp3\",\n",
    "#     \"\"\"You did an amazing job!\"\"\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/StyleTTS2/Demo/en-AU-Zak/zak2althaf-exclamation/eleven-labs/You did an amazing job!!!!!.mp3\",\n",
    "#     # \"\"\"Look out, there’s a car coming!\"\"\": reference,\n",
    "#     # \"\"\"You’ve got to try this recipe; it’s incredible!\"\"\": reference,\n",
    "#     # \"\"\"Let’s go on an adventure!\"\"\": reference,\n",
    "# }\n",
    "\n",
    "# synthesize_speech(reference_dicts, \"./Demo/en-AU-Zak/zak2althaf-exclamation/synthesized\", \"exclamation\", language=\"en-gb\", alpha=0.5, beta=0.0, embedding_scale=1)\n",
    "\n",
    "### QUESTION\n",
    "# reference = \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/StyleTTS2/Demo/en-AU-Zak/zak2althaf-question/eleven-labs/ElevenLabs_2024-11-01T15_51_57_zak-althaf_ivc_s81_sb100_se39_b_m2.mp3\"\n",
    "# reference_dicts = {\n",
    "#     \"\"\"Did anyone remember to check the schedule?\"\"\": reference,\n",
    "#     \"\"\"Could this really be the best solution?\"\"\": reference,\n",
    "#     \"\"\"Who can I reach out to if I have further questions?\"\"\": reference,\n",
    "#     \"\"\"How did you come up with that idea?\"\"\": reference,\n",
    "#     \"\"\"Should we double-check the details before finalising?\"\"\": reference,\n",
    "# }\n",
    "\n",
    "# synthesize_speech(reference_dicts, \"./Demo/en-AU-Zak/zak2althaf-question/synthesized\", \"zak_althaf_question\", language=\"en-gb\", alpha=0.3, beta=0.7, embedding_scale=1)\n",
    "\n",
    "# SINGLE WORD\n",
    "# reference = \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/StyleTTS2/Demo/en-AU-Zak/eleven_labs_zak2althaf_reference.mp3\"\n",
    "# reference_dicts = {\"suddenly.\": reference}\n",
    "\n",
    "# Short Sentence\n",
    "short_sentence = pd.read_csv(\"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/eleven_labs/audio/zak-althaf-short-sentences/metadata.csv\")\n",
    "\n",
    "reference_dicts = {}\n",
    "for datum in short_sentence[:20].iterrows():\n",
    "    text = datum[1][\"text\"]\n",
    "    path_to_audio = datum[1][\"path_to_audio\"]\n",
    "    path_to_audio = \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/eleven_labs/audio/zak-althaf-short-sentences/\" + path_to_audio\n",
    "    reference_dicts[text] =path_to_audio\n",
    "\n",
    "synthesize_speech(reference_dicts, \"./Demo/en-AU-Zak/zak2althaf-short-sentences/synthesized\", \"zak_althaf\", language=\"en-gb\", alpha=0.5, beta=0.3, embedding_scale=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Emphasis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_dicts = {\n",
    "    \"\"\" kəŋ\"ɡɹˈat\"ʃʊlˈeɪʃən \"ˌɒn\" \"jɔːɹ\" ɐt\"ʃˈiːv\"mənt!!! \"\"\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/StyleTTS2/Demo/en-AU-Zak/eleven_labs_zak2althaf_reference.mp3\",\n",
    "    \"\"\" \"aɪ\" \"kˈɑːnt\" bɪ\"lˈiːv\" \"juː\" \"dˈɪd\" \"ɪt!!!\" \"\"\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/StyleTTS2/Demo/en-AU-Zak/eleven_labs_zak2althaf_reference.mp3\",\n",
    "    # \"\"\"lˈʊk ˈaʊt, ðeəz ɐ kˈɑː kˈʌmɪŋ!  \"\"\": \"/home/s44504/StyleTTS2/Demo/en-AU-Zak/en-AU-Zak_0.wav\",\n",
    "    # \"\"\"juːv ɡɒt tə tɹˈaɪ ðɪs ɹˈɛsɪpˌiː; ɪts ɪŋkɹˈɛdɪbəl!  \"\"\": \"/home/s44504/StyleTTS2/Demo/en-AU-Zak/en-AU-Zak_0.wav\",\n",
    "    # \"\"\"lˈɛts ɡˌəʊ ˌɒn ɐn ɐdvˈɛntʃə!  \"\"\": \"/home/s44504/StyleTTS2/Demo/en-AU-Zak/en-AU-Zak_0.wav\",\n",
    "}\n",
    "\n",
    "synthesize_speech(reference_dicts, \"./Demo/en-AU-Zak/zak2althaf-emphasis\", \"\", language=\"en-gb\", alpha=0.7, beta=0.9, embedding_scale=1, phonemes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_dicts = {\n",
    "    \"\"\" ðɪs ɪz wɪðˌaʊt ˈɛmfəsɪs. \"ðɪs\" ɪz \"wɪð\" \"ˈɛm\"fəsɪs. \"\"\": \"/home/s44504/StyleTTS2/Demo/en-AU-Zak/concatenated_audio.wav\",\n",
    "    \"\"\"aɪ kəm\"plˈiːt\"li ɐ\"ɡɹˈiː\" wɪð juː.\"\"\": \"/home/s44504/StyleTTS2/Demo/en-AU-Zak/Zak2Althaf/Althaf-Zak_1.wav\",\n",
    "}\n",
    "\n",
    "synthesize_speech(reference_dicts, \"emphasis-althaf-zak\", \"emphasis-althaf-zak\", language=\"en-gb\", phonemes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID ALTHAF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps: lˈalu... \n",
      "RTF = 0.095099\n",
      "Audio saved to: ./Demo/id-ID-Althaf/synthesized/Lalu....wav\n",
      "ps: mˈuŋkin... ˈaku bˈisa bˈantu!kˈata bˈuruŋ.  bˈuruŋ lˈalu mˈinta ˌuntuk məndʒˈaɡa sˈiŋa ˈitu. zˈɛbra bˈoləh pˈɛrɡi mˈakan səkˈaraŋ.  bˈuruŋ sˈudah səlˈɛsaɪ mˈakan dan mˈasih kəɲˈaŋ səkˈali. \n",
      "RTF = 0.079740\n",
      "Audio saved to: ./Demo/id-ID-Althaf/synthesized/'Mungkin... aku bisa bantu!' kata burung.  Burung lalu minta untuk menjaga singa itu. Zebra boleh pergi makan sekarang.  Burung sudah selesai makan dan masih kenyang sekali. .wav\n",
      "ps: \"bˈintik mˈɛrah ˈini mənˈular!\" kˈata kˈami kəpˈada ˈadən. \n",
      "RTF = 0.384992\n",
      "Audio saved to: ./Demo/id-ID-Althaf/synthesized/\"Bintik merah ini menular!\" kata kami kepada Aden. .wav\n",
      "ps: bˈuku ˈini adˌalah bˈuku nˈonfˈiksi. \n",
      "RTF = 0.707816\n",
      "Audio saved to: ./Demo/id-ID-Althaf/synthesized/Buku ini adalah buku Non-Fiksi..wav\n",
      "ps: ˌinfɔrmˈasi. \n",
      "RTF = 1.666144\n",
      "Audio saved to: ./Demo/id-ID-Althaf/synthesized/Informasi..wav\n",
      "ps: kˈalaʊ bəɡˈitu... \n",
      "RTF = 1.935199\n",
      "Audio saved to: ./Demo/id-ID-Althaf/synthesized/Kalau begitu....wav\n",
      "ps: ˈulat bˈulu hˈarus məŋalˈami fˈasəfˈasə bərɡˈanti kˈulit ˌataʊ ˈinstar. lˈalu, mərˈɛka ˈakan bərtrˌansfɔrmˈasi məndʒˈadi kəpˈompoŋ. \n",
      "RTF = 0.484922\n",
      "Audio saved to: ./Demo/id-ID-Althaf/synthesized/Ulat bulu harus mengalami fase-fase berganti kulit atau instar. Lalu, mereka akan bertransformasi menjadi kepompong..wav\n"
     ]
    }
   ],
   "source": [
    "\n",
    "reference_dicts = {\n",
    "    \"\"\"Lalu...\"\"\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/StyleTTS2/Demo/id-ID-Althaf/concatenated_audio.wav\",\n",
    "    \"\"\"'Mungkin... aku bisa bantu!' kata burung.  Burung lalu minta untuk menjaga singa itu. Zebra boleh pergi makan sekarang.  Burung sudah selesai makan dan masih kenyang sekali. \"\"\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/StyleTTS2/Demo/id-ID-Althaf/concatenated_audio.wav\",\n",
    "    \"\"\"\"Bintik merah ini menular!\" kata kami kepada Aden. \"\"\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/StyleTTS2/Demo/id-ID-Althaf/concatenated_audio.wav\",\n",
    "    \"\"\"Buku ini adalah buku Non-Fiksi.\"\"\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/StyleTTS2/Demo/id-ID-Althaf/concatenated_audio.wav\",\n",
    "    \"\"\"Informasi.\"\"\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/StyleTTS2/Demo/id-ID-Althaf/concatenated_audio.wav\",\n",
    "    \"\"\"Kalau begitu...\"\"\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/StyleTTS2/Demo/id-ID-Althaf/concatenated_audio.wav\",\n",
    "    \"\"\"Ulat bulu harus mengalami fase-fase berganti kulit atau instar. Lalu, mereka akan bertransformasi menjadi kepompong.\"\"\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/StyleTTS2/Demo/id-ID-Althaf/concatenated_audio.wav\",\n",
    "}\n",
    "\n",
    "        \n",
    "        \n",
    "synthesize_speech(reference_dicts, \"./Demo/id-ID-Althaf/synthesized\", \"althaf\", language=\"id\", alpha=0.1, beta=0.3, embedding_scale=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesize_speech(reference_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SW Victoria\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"bookbot/OpenBible_Swahili\", \"JHN_clean\", num_proc=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import display, Audio\n",
    "# import soundfile as sf\n",
    "\n",
    "# def play_audio(example):\n",
    "#     audio = example['audio']\n",
    "#     display(Audio(audio['array'], rate=audio['sampling_rate']))\n",
    "    \n",
    "# play_audio(dataset[\"train\"][0])\n",
    "# output_directory = \"/home/s44504/StyleTTS2-clone/Demo/Swahili-Bible-John/original\"\n",
    "# os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "\n",
    "# for i, datum in enumerate(dataset[\"train\"]):\n",
    "#     if i == 6:\n",
    "#         break\n",
    "#     audio_sample = datum[\"audio\"]\n",
    "\n",
    "#         # Create the full path for the output file\n",
    "#     output_file_path = os.path.join(output_directory, f\"{datum['id']}.wav\")\n",
    "\n",
    "#     # Write the audio data to the file\n",
    "#     sf.write(output_file_path, audio_sample[\"array\"], audio_sample[\"sampling_rate\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps: habˈari za ukˈoo wa jˈesu krˈisto mwˈana wa daˈudi, mwˈana wa ˌabrahˈamu. \n",
      "RTF = 0.021256\n",
      "Audio saved to: ./Demo/Swahili-Bible/MAT/althaf/Habari za ukoo wa Yesu Kristo mwana wa Daudi, mwana wa Abrahamu..wav\n",
      "ps: ˌabrahˈamu ˌakamzˈaa isˈaki, isˈaki ˌakamzˈaa jakˈobo, jakˈobo ˌakawazˈaa jˈuda na ndˈuɡu zˈake. \n",
      "RTF = 0.041515\n",
      "Audio saved to: ./Demo/Swahili-Bible/MAT/althaf/Abrahamu akamzaa Isaki, Isaki akamzaa Yakobo, Yakobo akawazaa Yuda na ndugu zake..wav\n",
      "ps: jˈuda ˌakawazˈaa perˈesi na zˈera, ambˈao mˈama jˈao ˌalikˈuwa tamˈari, perˈesi ˌakamzˈaa hesrˈoni, hesrˈoni ˌakamzˈaa arˈamu. \n",
      "RTF = 0.063048\n",
      "Audio saved to: ./Demo/Swahili-Bible/MAT/althaf/Yuda akawazaa Peresi na Zera, ambao mama yao alikuwa Tamari, Peresi akamzaa Hesroni, Hesroni akamzaa Aramu..wav\n",
      "ps: arˈamu ˌakamzˈaa ˌaminadˈabu, ˌaminadˈabu ˌakamzˈaa naʃˈoni, naʃˈoni ˌakamzˈaa salmˈoni. \n",
      "RTF = 0.114912\n",
      "Audio saved to: ./Demo/Swahili-Bible/MAT/althaf/Aramu akamzaa Aminadabu, Aminadabu akamzaa Nashoni, Nashoni akamzaa Salmoni..wav\n",
      "ps: salmˈoni ˌakamzˈaa boˈazi, na mˈama jˌake boˈazi ˌalikˈuwa rahˈabu, boˈazi ˌakamzˈaa obˈedi, ambˈaje mˈama jˌake ˌalikˈuwa rˈuθu, obˈedi ˌakamzˈaa jˈese. \n",
      "RTF = 0.079248\n",
      "Audio saved to: ./Demo/Swahili-Bible/MAT/althaf/Salmoni akamzaa Boazi, na mama yake Boazi alikuwa Rahabu, Boazi akamzaa Obedi, ambaye mama yake alikuwa Ruthu, Obedi akamzaa Yese..wav\n"
     ]
    }
   ],
   "source": [
    "# # Testing sounds with 2 consonant cluster\n",
    "# reference_dicts = {\n",
    "#     \"\"\"Habari za ukoo wa Yesu Kristo mwana wa Daudi, mwana wa Abrahamu.\"\"\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/sw-TZ-Victoria/wavs/sw-TZ-Victoria_000026.wav\",\n",
    "#     \"\"\"Abrahamu akamzaa Isaki, Isaki akamzaa Yakobo, Yakobo akawazaa Yuda na ndugu zake.\"\"\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/sw-TZ-Victoria/wavs/sw-TZ-Victoria_000026.wav\",\n",
    "#     \"\"\"Yuda akawazaa Peresi na Zera, ambao mama yao alikuwa Tamari, Peresi akamzaa Hesroni, Hesroni akamzaa Aramu.\"\"\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/sw-TZ-Victoria/wavs/sw-TZ-Victoria_000026.wav\",\n",
    "#     \"\"\"Aramu akamzaa Aminadabu, Aminadabu akamzaa Nashoni, Nashoni akamzaa Salmoni.\"\"\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/sw-TZ-Victoria/wavs/sw-TZ-Victoria_000026.wav\",\n",
    "#     \"\"\"Salmoni akamzaa Boazi, na mama yake Boazi alikuwa Rahabu, Boazi akamzaa Obedi, ambaye mama yake alikuwa Ruthu, Obedi akamzaa Yese.\"\"\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/sw-TZ-Victoria/wavs/sw-TZ-Victoria_000026.wav\",\n",
    "# }\n",
    "reference_dicts = {\n",
    "    \"\"\"Habari za ukoo wa Yesu Kristo mwana wa Daudi, mwana wa Abrahamu.\"\"\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/OpenBible_Swahili_Althaf/MAT_clean/wavs/MAT_001_001_smooth.wav\",\n",
    "    \"\"\"Abrahamu akamzaa Isaki, Isaki akamzaa Yakobo, Yakobo akawazaa Yuda na ndugu zake.\"\"\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/OpenBible_Swahili_Althaf/MAT_clean/wavs/MAT_001_002_smooth.wav\",\n",
    "    \"\"\"Yuda akawazaa Peresi na Zera, ambao mama yao alikuwa Tamari, Peresi akamzaa Hesroni, Hesroni akamzaa Aramu.\"\"\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/OpenBible_Swahili_Althaf/MAT_clean/wavs/MAT_001_003_smooth.wav\",\n",
    "    \"\"\"Aramu akamzaa Aminadabu, Aminadabu akamzaa Nashoni, Nashoni akamzaa Salmoni.\"\"\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/OpenBible_Swahili_Althaf/MAT_clean/wavs/MAT_001_004_smooth.wav\",\n",
    "    \"\"\"Salmoni akamzaa Boazi, na mama yake Boazi alikuwa Rahabu, Boazi akamzaa Obedi, ambaye mama yake alikuwa Ruthu, Obedi akamzaa Yese.\"\"\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/OpenBible_Swahili_Althaf/MAT_clean/wavs/MAT_001_005_smooth.wav\",\n",
    "}\n",
    "\n",
    "# reference_dicts = {\n",
    "#     \"\"\"Ningependa kwenda shuleni leo. \"\"\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/OpenBible_Swahili_Althaf/MAT_clean/wavs/MAT_004_023_smooth.wav\",\n",
    "#     \"\"\"Mbwa wangu anapenda kula nyama. \"\"\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/OpenBible_Swahili_Althaf/MAT_clean/wavs/MAT_004_023_smooth.wav\",\n",
    "#     \"\"\"Ndizi zangu zimeiva vizuri. \"\"\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/OpenBible_Swahili_Althaf/MAT_clean/wavs/MAT_004_023_smooth.wav\",\n",
    "#     \"\"\"Mtoto anapiga ngoma kwa nguvu. \"\"\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/OpenBible_Swahili_Althaf/MAT_clean/wavs/MAT_004_023_smooth.wav\",\n",
    "#     \"\"\"Mboga za kijani ni nzuri kwa afya. \"\"\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/OpenBible_Swahili_Althaf/MAT_clean/wavs/MAT_004_023_smooth.wav\",\n",
    "#     \"\"\"Tonge la ugali ni tamu. \"\"\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/OpenBible_Swahili_Althaf/MAT_clean/wavs/MAT_004_023_smooth.wav\",\n",
    "# }\n",
    "\n",
    "synthesize_speech(reference_dicts, \"./Demo/Swahili-Bible/MAT/althaf\", \"althaf\", language=\"sw\", alpha=0.7, beta=0.5, embedding_scale=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S2S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import phonemizer\n",
    "\n",
    "def s2s(\n",
    "    text,\n",
    "    ref_s,\n",
    "    target_s,\n",
    "    language,\n",
    "    alpha=0.8,\n",
    "    beta=0.1,\n",
    "    diffusion_steps=10,\n",
    "    embedding_scale=1,\n",
    "    phonemes=False,\n",
    "):\n",
    "    global_phonemizer = phonemizer.backend.EspeakBackend(\n",
    "        language=language, preserve_punctuation=True, with_stress=True\n",
    "    )\n",
    "\n",
    "    text = text.strip()\n",
    "    if phonemes:\n",
    "        ps = text\n",
    "    else:\n",
    "        ps = global_phonemizer.phonemize([text])[0]\n",
    "    print(f\"ps: {ps}\")\n",
    "    # ps = word_tokenize(ps[0])\n",
    "    # ps = \" \".join(ps)\n",
    "    tokens = textcleaner(ps)\n",
    "    tokens.insert(0, 0)\n",
    "    tokens = torch.LongTensor(tokens).to(device).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_lengths = torch.LongTensor([tokens.shape[-1]]).to(device)\n",
    "        text_mask = length_to_mask(input_lengths).to(device)\n",
    "\n",
    "        t_en = model.text_encoder(tokens, input_lengths, text_mask)\n",
    "        bert_dur = model.bert(tokens, attention_mask=(~text_mask).int())\n",
    "        d_en = model.bert_encoder(bert_dur).transpose(-1, -2)\n",
    "\n",
    "        s_pred = sampler(\n",
    "            noise=torch.randn((1, 256)).unsqueeze(1).to(device),\n",
    "            embedding=bert_dur,\n",
    "            embedding_scale=embedding_scale,\n",
    "            features=target_s,  # reference from the same speaker as the embedding\n",
    "            num_steps=diffusion_steps,\n",
    "        ).squeeze(1)\n",
    "\n",
    "        ref = s_pred[:, :128]\n",
    "        s = s_pred[:, 128:]\n",
    "        \n",
    "        # Ref depebds on target_styke\n",
    "        ref = alpha * ref + (1 - alpha) * ref_s[:, :128]\n",
    "        s = beta * s + (1 - beta) * ref_s[:, 128:]\n",
    "\n",
    "        d = model.predictor.text_encoder(d_en, s, input_lengths, text_mask)\n",
    "\n",
    "        x, _ = model.predictor.lstm(d)\n",
    "        duration = model.predictor.duration_proj(x)\n",
    "\n",
    "        duration = torch.sigmoid(duration).sum(axis=-1)\n",
    "        pred_dur = torch.round(duration.squeeze()).clamp(min=1)\n",
    "\n",
    "        pred_aln_trg = torch.zeros(input_lengths, int(pred_dur.sum().data))\n",
    "        c_frame = 0\n",
    "        for i in range(pred_aln_trg.size(0)):\n",
    "            pred_aln_trg[i, c_frame : c_frame + int(pred_dur[i].data)] = 1\n",
    "            c_frame += int(pred_dur[i].data)\n",
    "\n",
    "        # encode prosody\n",
    "        en = d.transpose(-1, -2) @ pred_aln_trg.unsqueeze(0).to(device)\n",
    "        if model_params.decoder.type == \"hifigan\":\n",
    "            asr_new = torch.zeros_like(en)\n",
    "            asr_new[:, :, 0] = en[:, :, 0]\n",
    "            asr_new[:, :, 1:] = en[:, :, 0:-1]\n",
    "            en = asr_new\n",
    "\n",
    "        F0_pred, N_pred = model.predictor.F0Ntrain(en, s)\n",
    "\n",
    "        asr = t_en @ pred_aln_trg.unsqueeze(0).to(device)\n",
    "        if model_params.decoder.type == \"hifigan\":\n",
    "            asr_new = torch.zeros_like(asr)\n",
    "            asr_new[:, :, 0] = asr[:, :, 0]\n",
    "            asr_new[:, :, 1:] = asr[:, :, 0:-1]\n",
    "            asr = asr_new\n",
    "\n",
    "        out = model.decoder(asr, F0_pred, N_pred, ref.squeeze().unsqueeze(0))\n",
    "\n",
    "    return out.squeeze().cpu().numpy()[..., :-50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ps: ɪt sˈɛd nˈəʊ. \n",
      "ps: ɐ wˈʊlf kˈʌmz. \n",
      "ps: ɐn ʌmbɹˈɛlə? \n",
      "ps: ðə kˈat ɹˈan. \n",
      "ps: dˈaɹən sˈɛd. \n",
      "ps: mˈʌm ɪz ɡlˈad. \n",
      "ps: ðə mˈan nˈɒdɪd. \n",
      "ps: zˈɒmbi ɐtˈak. \n",
      "ps: ɐ spˈɛʃəl flˈaʊə. \n",
      "ps: ɐ fˈɑːm dˈɒɡ. \n",
      "ps: ɪt lˈandz ˈɒn. \n",
      "ps: ɪt hˈɪt mˌiː. \n",
      "ps: tˈɪk ɪz sˈeɪf! \n",
      "ps: ɐ bˈɪɡ mˈɛs! \n",
      "ps: flˈaʊə hˈʌnt. \n",
      "ps: pˈat ðə kˈat. \n",
      "ps: hˈeɪ, pˈat! \n",
      "ps: jˈuːv ˈɜːnd ɪt. \n",
      "ps: ɪt swˈɪŋz. \n",
      "ps: wiː ɑː fˈɪʃ. \n",
      "ps: fˈʊl mˈɛtəl nˈɛd. \n",
      "ps: pˈʌpɪz bˈɜːθdeɪ. \n",
      "ps: maɪ bˈɛd? \n",
      "ps: ɪts ˌəʊkˈeɪ, ˈɛlɪfənt. \n"
     ]
    }
   ],
   "source": [
    "### Angry\n",
    "# reference_dicts = {\n",
    "#     # \"StyleTTS 2 is a text to speech model that leverages style diffusion and adversarial training with large speech language models to achieve human level text to speech synthesis.\" : \"/home/s44504/StyleTTS2-clone/Demo/en-US-Madison/madison2althaf/madison2althaf_0.wav\",\n",
    "#     # \"I am very angry at him! He pisses me off\" : \"/home/s44504/StyleTTS2-clone/Demo/en-US-Madison/madison2althaf/madison2althaf_1.wav\",\n",
    "#     \"The field of astronomy is a joke! Its theories are based on flawed observations!\" : \"/home/s44504/StyleTTS2-clone/Demo/en-US-Madison/Angry-1.wav\",\n",
    "#     \"This is absolutely unacceptable, and I won’t tolerate it any longer!\" : \"/home/s44504/StyleTTS2-clone/Demo/en-US-Madison/Angry-3.wav\",\n",
    "#     \"Not at this particular case, Tom, apologized Whittemore.\" : \"/home/s44504/StyleTTS2-clone/Demo/jeanie-Angry/amused_1-28_0002.wav\",\n",
    "# }\n",
    "\n",
    "# ### Thalia\n",
    "# reference_dicts = {\n",
    "#     \"\"\" StyleTTS 2 is a text to speech model that leverages style diffusion and adversarial training with large speech language models to achieve human level text to speech synthesis. \"\"\": \"/home/s44504/StyleTTS2-clone/Demo/en-UK-Thalia/thalia/en-thalia_0.wav\",\n",
    "#     \"\"\" Then. whoosh! It takes off! \"\"\": \"/home/s44504/StyleTTS2-clone/Demo/en-UK-Thalia/thalia/en-thalia_1.wav\",\n",
    "#     \"\"\" So he is convinced he can fix that. So he goes to work creating a red hut here. \"\"\": \"/home/s44504/StyleTTS2-clone/Demo/en-UK-Thalia/thalia/en-thalia_2.wav\",\n",
    "#     \"\"\" This is Forester. He works out in the forest. \"\"\": \"/home/s44504/StyleTTS2-clone/Demo/en-UK-Thalia/thalia/en-thalia_3.wav\",\n",
    "#     \"\"\" It brings him a lot of happiness to work with animals. \"\"\": \"/home/s44504/StyleTTS2-clone/Demo/en-UK-Thalia/thalia/en-thalia_4.wav\",\n",
    "#     \"\"\" He checks on them every day to make sure that they are doing well. \"\"\": \"/home/s44504/StyleTTS2-clone/Demo/en-UK-Thalia/thalia/en-thalia_5.wav\",\n",
    "# }\n",
    "\n",
    "### Zak\n",
    "# reference_dicts = {\n",
    "    # \"\"\"Hi there!\"\"\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/StyleTTS2/Demo/en-AU-Zak/zak/Hi there!.wav\",\n",
    "    # \"\"\" Then, whoosh! It takes off! \"\"\": \"/home/s44504/StyleTTS2-clone/Demo/en-AU-Zak/zak/en-zak_1.wav\",\n",
    "    # \"\"\" So he is convinced he can fix that. So he goes to work creating a red hut here. \"\"\": \"/home/s44504/StyleTTS2-clone/Demo/en-AU-Zak/zak/en-zak_2.wav\",\n",
    "    # \"\"\" This is Forester. He works out in the forest. \"\"\": \"/home/s44504/StyleTTS2-clone/Demo/en-AU-Zak/zak/en-zak_3.wav\",\n",
    "    # \"\"\" It brings him a lot of happiness to work with animals. \"\"\": \"/home/s44504/StyleTTS2-clone/Demo/en-AU-Zak/zak/en-zak_4.wav\",\n",
    "    # \"\"\" He checks on them every day to make sure that they are doing well. \"\"\": \"/home/s44504/StyleTTS2-clone/Demo/en-AU-Zak/zak/en-zak_5.wav\",\n",
    "    # \"\"\" Diya never wants to talk in class. \"\"\": \"/home/s44504/StyleTTS2-clone/Demo/en-AU-Zak/zak/en-zak_6.wav\",\n",
    "    # }\n",
    "\n",
    "# Madison\n",
    "# reference_dicts = {\n",
    "#    \"Learning is a journey filled with curiosity, wonder, and discovery. Every child deserves the opportunity to explore and grow at their own pace. By fostering a love for reading and numbers, we empower them to reach their fullest potential.\" : \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/StyleTTS2/Demo/en-US-Madison/madison/madison_reference.wav\",\n",
    "# }\n",
    "\n",
    "\n",
    "### Swahili\n",
    "# reference_dicts = {\n",
    "#     \"\"\"Habari za ukoo wa Yesu Kristo mwana wa Daudi, mwana wa Abrahamu.\"\"\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/OpenBible_Swahili_Althaf/MAT_clean/wavs/MAT_001_001_smooth.wav\",\n",
    "#     \"\"\"Abrahamu akamzaa Isaki, Isaki akamzaa Yakobo, Yakobo akawazaa Yuda na ndugu zake.\"\"\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/OpenBible_Swahili_Althaf/MAT_clean/wavs/MAT_001_002_smooth.wav\",\n",
    "#     \"\"\"Yuda akawazaa Peresi na Zera, ambao mama yao alikuwa Tamari, Peresi akamzaa Hesroni, Hesroni akamzaa Aramu.\"\"\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/OpenBible_Swahili_Althaf/MAT_clean/wavs/MAT_001_003_smooth.wav\",\n",
    "#     \"\"\"\"Aramu akamzaa Aminadabu, Aminadabu akamzaa Nashoni, Nashoni akamzaa Salmoni.\"\"\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/OpenBible_Swahili_Althaf/MAT_clean/wavs/MAT_001_004_smooth.wav\",\n",
    "#     \"\"\"a. sa. nte.\"\"\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/sw-TZ-Althaf-Syllables/wavs/sw-TZ-Victoria_syllable_1150_0.wav\",\n",
    "# }\n",
    "\n",
    "# reference_dicts = {\n",
    "#     \"\"\"Ningependa kwenda shuleni leo. \"\"\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/StyleTTS2/Demo/Swahili-Bible-John/synthesized/ Ningependa kwenda shuleni leo. .wav\",\n",
    "#     \"\"\"Mbwa wangu anapenda kula nyama. \"\"\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/StyleTTS2/Demo/Swahili-Bible-John/synthesized/ Mbwa wangu anapenda kula nyama. .wav\",\n",
    "#     \"\"\"Ndizi zangu zimeiva vizuri. \"\"\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/StyleTTS2/Demo/Swahili-Bible-John/synthesized/ Ndizi zangu zimeiva vizuri. .wav\",\n",
    "#     \"\"\"Mtoto anapiga ngoma kwa nguvu. \"\"\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/StyleTTS2/Demo/Swahili-Bible-John/synthesized/ Mtoto anapiga ngoma kwa nguvu. .wav\",\n",
    "#     \"\"\"Mboga za kijani ni nzuri kwa afya. \"\"\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/StyleTTS2/Demo/Swahili-Bible-John/synthesized/ Mboga za kijani ni nzuri kwa afya. .wav\",\n",
    "#     \"\"\"Tonge la ugali ni tamu. \"\"\": \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/StyleTTS2/Demo/Swahili-Bible-John/synthesized/ Tonge la ugali ni tamu. .wav\",\n",
    "# }\n",
    "\n",
    "# Short Sentence\n",
    "import pandas as pd\n",
    "short_sentence = pd.read_csv(\"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/eleven_labs/audio/zak-althaf-short-sentences/metadata.csv\")\n",
    "\n",
    "reference_dicts = {}\n",
    "for datum in short_sentence[:30].iterrows():\n",
    "    text = datum[1][\"text\"]\n",
    "    path_to_audio = datum[1][\"path_to_audio\"]\n",
    "    path_to_audio = \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/eleven_labs/audio/zak-althaf-short-sentences/\" + path_to_audio\n",
    "    reference_dicts[text] = path_to_audio\n",
    "\n",
    "ref_althaf = compute_style(\n",
    "    \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/StyleTTS2/Demo/id-ID-Althaf/concatenated_audio.wav\"\n",
    ")\n",
    "\n",
    "idx = 0\n",
    "for text, ref_path in reference_dicts.items():\n",
    "    try:\n",
    "        ref_path = compute_style(ref_path)\n",
    "        wav = s2s(\n",
    "            text,\n",
    "            ref_path,\n",
    "            ref_althaf,\n",
    "            \"en-gb\",\n",
    "            alpha=0.6,\n",
    "            beta=0.2,\n",
    "            diffusion_steps=10,\n",
    "            phonemes=False\n",
    "        )\n",
    "    except Exception as e:\n",
    "        continue\n",
    "    \n",
    "    # Apply fadeout (50ms)\n",
    "    fade_length = int(SAMPLING_RATE * 0.1)  # 50ms\n",
    "    fade_curve = np.linspace(1.0, 0.0, fade_length)\n",
    "    wav[-fade_length:] *= fade_curve\n",
    "    \n",
    "    # Add 100ms silence\n",
    "    silence_length = int(SAMPLING_RATE * 0.1)  # 100ms\n",
    "    silence = np.zeros(silence_length)\n",
    "    wav = np.concatenate([wav, silence])\n",
    "    \n",
    "    target_dir = Path(\"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/StyleTTS2/Demo/en-AU-Zak/zak2althaf-short-sentences/speech2speech\")\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    target_path = f\"{target_dir}/{text}.wav\"\n",
    "    sf.write(target_path, wav, SAMPLING_RATE)\n",
    "    from audiostretchy.stretch import stretch_audio\n",
    "    stretch_audio(target_path, target_path, ratio=0.8)\n",
    "    \n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "audio_path = \"/home/s44504/3b01c699-3670-469b-801f-13880b9cac56/en-Multi-Exclamation-24kHz/wavs/en-AU-Zak_line_0001.wav\"\n",
    "stretch_audio(audio_path, \"output.wav\", ratio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_dicts = {\n",
    "    \" His name is Archie. He likes to make stuff.  Archie works all of the time. He has made bots to help him in his home. He has made a bot to do art. He must be fit to work well. He rides his bike. Archie made a van that helps.  He can use cogs and tubes to make wild bits and bobs. Will Archie make bots to help or for fun next? \" : \"/home/s44504/StyleTTS2/Demo/en-AU-Zak/zak/en-zak_0.wav\",\n",
    "    # \"StyleTTS 2 is a text to speech model that leverages style diffusion and adversarial training with large speech language models to achieve human level text to speech synthesis.\" : \"/home/s44504/StyleTTS2-clone/Demo/en-US-Madison/madison/madison_template_text_0.wav\",\n",
    "    # \"I am very angry at him! He pisses me off\" : \"/home/s44504/StyleTTS2-clone/Demo/en-US-Madison/madison/madison_0.wav\",\n",
    "    # \"The field of astronomy is a joke! Its theories are based on flawed observations!\" : \"/home/s44504/StyleTTS2-clone/Demo/en-US-Madison/madison/madison_1.wav\",\n",
    "    # \"This is absolutely unacceptable, and I won’t tolerate it any longer!\" : \"/home/s44504/StyleTTS2-clone/Demo/en-US-Madison/madison/madison_2.wav\"\n",
    "}\n",
    "\n",
    "ref_althaf = compute_style(\n",
    "    \"/home/s44504/StyleTTS2/Demo/id-ID-Althaf/concatenated_audio.wav\"\n",
    ")\n",
    "\n",
    "idx = 0\n",
    "for text, ref_path in reference_dicts.items():\n",
    "    ref_path = compute_style(ref_path)\n",
    "    wav = s2s(\n",
    "        text,\n",
    "        ref_path,\n",
    "        ref_althaf,\n",
    "        \"en-gb\",\n",
    "        alpha=0.7,\n",
    "        beta=0.3,\n",
    "        diffusion_steps=10,\n",
    "        phonemes=False,\n",
    "    )\n",
    "    \n",
    "    target_dir = Path(\"Demo/en-AU-Zak/zak2althaf\")\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    sf.write(f\"{target_dir}/zak2althaf_{idx}.wav\", wav, SAMPLING_RATE)\n",
    "    idx += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
