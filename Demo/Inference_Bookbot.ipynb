{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HF_HOME\"] = (\n",
    "    \"/media/s44504/3b01c699-3670-469b-801f-13880b9cac56/huggingface/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# dataset_list = [\"en-US-Madison\", \"en-AU-Zak\", \"id-ID-Althaf\"] # range(0, 5)\n",
    "# dataset_list = [\"en-UK-Thalia\"] (range(47, 57))\n",
    "dataset_list = [\"sw-TZ-Victoria\"]\n",
    "\n",
    "\n",
    "for dataset_uri in dataset_list:\n",
    "    # dataset = load_dataset(\"bookbot/id-ID-Althaf\", num_proc=os.cpu_count)\n",
    "    dataset = load_dataset(\"bookbot/sw-TZ-Victoria\", num_proc=os.cpu_count)\n",
    "    # dataset = dataset.filter(lambda example: example[\"speaker\"] == dataset_uri)\n",
    "    dataset = dataset[\"train\"].select(range(0, 5))\n",
    "    dataset_name = Path(dataset_uri).stem\n",
    "\n",
    "    # Specify the directory where you want to save the WAV files\n",
    "    output_directory = f\"{dataset_name}\"\n",
    "\n",
    "    # Ensure the output directory exists\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "    # Initialize an empty list to hold the audio data arrays\n",
    "    concatenated_audio = []\n",
    "\n",
    "    # Iterate over each item in the dataset\n",
    "    for i, item in enumerate(dataset):\n",
    "        # Extract the audio array and sampling rate\n",
    "        audio_array = item[\"audio\"][\"array\"]\n",
    "        sampling_rate = item[\"audio\"][\"sampling_rate\"]\n",
    "        try:\n",
    "            id = item[\"id\"]\n",
    "        except:\n",
    "            id = i\n",
    "\n",
    "        # Define the output file path\n",
    "        output_file_path = os.path.join(output_directory, f\"{id}.wav\")\n",
    "\n",
    "        # Write the audio data to a WAV file\n",
    "        sf.write(output_file_path, audio_array, sampling_rate)\n",
    "\n",
    "        # Append the audio data to the list\n",
    "        concatenated_audio.append(audio_array)\n",
    "\n",
    "        silence_length_samples = int((350 / 1000) * sampling_rate)\n",
    "        silence_audio = np.zeros(silence_length_samples)\n",
    "        concatenated_audio.append(silence_audio)\n",
    "\n",
    "        print(f\"Written file {output_file_path}\")\n",
    "\n",
    "    # Concatenate the audio data arrays\n",
    "    concatenated_audio = np.concatenate(concatenated_audio)\n",
    "\n",
    "    # Define the output file path for the concatenated audio\n",
    "    concatenated_file_path = os.path.join(output_directory, \"concatenated_audio.wav\")\n",
    "\n",
    "    # Write the concatenated audio data to a WAV file\n",
    "    sf.write(concatenated_file_path, concatenated_audio, sampling_rate)\n",
    "\n",
    "    print(f\"Concatenated audio written to {concatenated_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/s44504/StyleTTS2\n",
      "177\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "%cd ..\n",
    "\n",
    "# load packages\n",
    "import time\n",
    "import random\n",
    "import yaml\n",
    "from munch import Munch\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import librosa\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from models import *\n",
    "from utils import *\n",
    "from text_utils import TextCleaner\n",
    "textclenaer = TextCleaner()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_mel = torchaudio.transforms.MelSpectrogram(\n",
    "    n_mels=80, n_fft=2048, win_length=1200, hop_length=300\n",
    ")\n",
    "mean, std = -4, 4\n",
    "\n",
    "\n",
    "def length_to_mask(lengths):\n",
    "    mask = (\n",
    "        torch.arange(lengths.max())\n",
    "        .unsqueeze(0)\n",
    "        .expand(lengths.shape[0], -1)\n",
    "        .type_as(lengths)\n",
    "    )\n",
    "    mask = torch.gt(mask + 1, lengths.unsqueeze(1))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def preprocess(wave):\n",
    "    wave_tensor = torch.from_numpy(wave).float()\n",
    "    mel_tensor = to_mel(wave_tensor)\n",
    "    mel_tensor = (torch.log(1e-5 + mel_tensor.unsqueeze(0)) - mean) / std\n",
    "    return mel_tensor\n",
    "\n",
    "\n",
    "def compute_style(path):\n",
    "    wave, sr = librosa.load(path, sr=24000)\n",
    "    audio, index = librosa.effects.trim(wave, top_db=30)\n",
    "    if sr != 24000:\n",
    "        audio = librosa.resample(audio, sr, 24000)\n",
    "    mel_tensor = preprocess(audio).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ref_s = model.style_encoder(mel_tensor.unsqueeze(1))\n",
    "        ref_p = model.predictor_encoder(mel_tensor.unsqueeze(1))\n",
    "\n",
    "    return torch.cat([ref_s, ref_p], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s44504/miniconda3/envs/vad/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/s44504/miniconda3/envs/vad/lib/python3.9/site-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "/home/s44504/miniconda3/envs/vad/lib/python3.9/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# load phonemizer\n",
    "import phonemizer\n",
    "\n",
    "global_phonemizer = phonemizer.backend.EspeakBackend(\n",
    "    language=\"sw\", preserve_punctuation=True, with_stress=True\n",
    ")\n",
    "\n",
    "# config = yaml.safe_load(open(\"Models/EN-Multi-ID-Althaf/config_ft_en_multi_id_althaf.yml\"))\n",
    "config = yaml.safe_load(open(\"Models/EN-Multi-ID-Althaf-SW-Victoria/config_ft_en_multi_id_althaf_sw_victoria.yml\"))\n",
    "\n",
    "# load pretrained ASR model\n",
    "ASR_config = config.get(\"ASR_config\", False)\n",
    "ASR_path = config.get(\"ASR_path\", False)\n",
    "text_aligner = load_ASR_models(ASR_path, ASR_config)\n",
    "\n",
    "# load pretrained F0 model\n",
    "F0_path = config.get(\"F0_path\", False)\n",
    "pitch_extractor = load_F0_models(F0_path)\n",
    "\n",
    "# load BERT model\n",
    "from Utils.PLBERT.util import load_plbert\n",
    "\n",
    "BERT_path = config.get(\"PLBERT_dir\", False)\n",
    "plbert = load_plbert(BERT_path)\n",
    "\n",
    "model_params = recursive_munch(config[\"model_params\"])\n",
    "model = build_model(model_params, text_aligner, pitch_extractor, plbert)\n",
    "_ = [model[key].eval() for key in model]\n",
    "_ = [model[key].to(device) for key in model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params_whole = torch.load(\"Models/EN-Multi-ID-Althaf/epoch_2nd_00024.pth\", map_location=\"cpu\")\n",
    "params_whole = torch.load(\"Models/EN-Multi-ID-Althaf-SW-Victoria/epoch_2nd_00019.pth\", map_location=\"cpu\")\n",
    "# params_whole = torch.load(\"Models/LibriTTS/epochs_2nd_00020.pth\", map_location=\"cpu\")\n",
    "params = params_whole[\"net\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert loaded\n",
      "bert_encoder loaded\n",
      "predictor loaded\n",
      "decoder loaded\n",
      "text_encoder loaded\n",
      "predictor_encoder loaded\n",
      "style_encoder loaded\n",
      "diffusion loaded\n",
      "text_aligner loaded\n",
      "pitch_extractor loaded\n",
      "mpd loaded\n",
      "msd loaded\n",
      "wd loaded\n"
     ]
    }
   ],
   "source": [
    "for key in model:\n",
    "    if key in params:\n",
    "        print(\"%s loaded\" % key)\n",
    "        try:\n",
    "            model[key].load_state_dict(params[key])\n",
    "        except:\n",
    "            from collections import OrderedDict\n",
    "\n",
    "            state_dict = params[key]\n",
    "            new_state_dict = OrderedDict()\n",
    "            for k, v in state_dict.items():\n",
    "                name = k[7:]  # remove `module.`\n",
    "                new_state_dict[name] = v\n",
    "            # load params\n",
    "            model[key].load_state_dict(new_state_dict, strict=False)\n",
    "#             except:\n",
    "#                 _load(params[key], model[key])\n",
    "_ = [model[key].eval() for key in model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Modules.diffusion.sampler import DiffusionSampler, ADPM2Sampler, KarrasSchedule\n",
    "\n",
    "sampler = DiffusionSampler(\n",
    "    model.diffusion.diffusion,\n",
    "    sampler=ADPM2Sampler(),\n",
    "    sigma_schedule=KarrasSchedule(\n",
    "        sigma_min=0.0001, sigma_max=3.0, rho=9.0\n",
    "    ),  # empirical parameters\n",
    "    clamp=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"Now he cannot find his way back home.\"\n",
    "# text = \"I am so glad that I have a mom and dad to get through everything in my day.\"\n",
    "# text = \"kila mmoja alichukua kipande chake na kuhesabu sehemu zake kwa makini\"\n",
    "text = text.strip()\n",
    "ps = global_phonemizer.phonemize([text])\n",
    "ps = word_tokenize(ps[0])\n",
    "ps = \" \".join(ps)\n",
    "tokens = textclenaer(ps)\n",
    "tokens.insert(0, 0)\n",
    "tokens = torch.LongTensor(tokens).to(device).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.8\n",
    "beta = 0.1\n",
    "diffusion_steps = 10\n",
    "embedding_scale = 1.0\n",
    "\n",
    "ref_thalia = compute_style(\"/home/s44504/StyleTTS2/Demo/en-UK-Thalia/en-UK-Thalia_141.wav\")\n",
    "# ref_victoria = compute_style(\"/home/s44504/StyleTTS2/Demo/sw-TZ-Victoria/sw-TZ-Victoria_Gawa_chungwa_3.wav\")\n",
    "ref_althaf = compute_style(\"/home/s44504/StyleTTS2/Demo/id-ID-Althaf/concatenated_audio.wav\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    input_lengths = torch.LongTensor([tokens.shape[-1]]).to(device)\n",
    "    text_mask = length_to_mask(input_lengths).to(device)\n",
    "\n",
    "    t_en = model.text_encoder(tokens, input_lengths, text_mask)\n",
    "    bert_dur = model.bert(tokens, attention_mask=(~text_mask).int())\n",
    "    d_en = model.bert_encoder(bert_dur).transpose(-1, -2)\n",
    "\n",
    "    s_pred = sampler(\n",
    "        noise=torch.randn((1, 256)).unsqueeze(1).to(device),\n",
    "        embedding=bert_dur,\n",
    "        embedding_scale=embedding_scale,\n",
    "        features=ref_althaf,  # reference from the same speaker as the embedding\n",
    "        num_steps=diffusion_steps,\n",
    "    ).squeeze(1)\n",
    "\n",
    "    ref = s_pred[:, :128]\n",
    "    s = s_pred[:, 128:]\n",
    "\n",
    "    ref = alpha * ref + (1 - alpha) * ref_thalia[:, :128]\n",
    "    s = beta * s + (1 - beta) * ref_thalia[:, 128:]\n",
    "\n",
    "    d = model.predictor.text_encoder(d_en, s, input_lengths, text_mask)\n",
    "\n",
    "    x, _ = model.predictor.lstm(d)\n",
    "    duration = model.predictor.duration_proj(x)\n",
    "\n",
    "    duration = torch.sigmoid(duration).sum(axis=-1)\n",
    "    pred_dur = torch.round(duration.squeeze()).clamp(min=1)\n",
    "\n",
    "    pred_aln_trg = torch.zeros(input_lengths, int(pred_dur.sum().data))\n",
    "    c_frame = 0\n",
    "    for i in range(pred_aln_trg.size(0)):\n",
    "        pred_aln_trg[i, c_frame : c_frame + int(pred_dur[i].data)] = 1\n",
    "        c_frame += int(pred_dur[i].data)\n",
    "\n",
    "    # encode prosody\n",
    "    en = d.transpose(-1, -2) @ pred_aln_trg.unsqueeze(0).to(device)\n",
    "    if model_params.decoder.type == \"hifigan\":\n",
    "        asr_new = torch.zeros_like(en)\n",
    "        asr_new[:, :, 0] = en[:, :, 0]\n",
    "        asr_new[:, :, 1:] = en[:, :, 0:-1]\n",
    "        en = asr_new\n",
    "\n",
    "    # F0_pred, N_pred = model.predictor.F0Ntrain(en, ref_thalia[:, 128:])\n",
    "    # F0_pred, N_pred = model.predictor.F0Ntrain(en, ref_althaf[:, 128:])\n",
    "    F0_pred, N_pred = model.predictor.F0Ntrain(en, s)\n",
    "\n",
    "    # F0_real, _, F0 = model.pitch_extractor(mel_tensor.unsqueeze(1))\n",
    "    # F0_real = F0_real.unsqueeze(0)\n",
    "    # N_real = log_norm(mel_tensor.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "    asr = t_en @ pred_aln_trg.unsqueeze(0).to(device)\n",
    "    if model_params.decoder.type == \"hifigan\":\n",
    "        asr_new = torch.zeros_like(asr)\n",
    "        asr_new[:, :, 0] = asr[:, :, 0]\n",
    "        asr_new[:, :, 1:] = asr[:, :, 0:-1]\n",
    "        asr = asr_new\n",
    "\n",
    "    out = model.decoder(asr, F0_pred, N_pred, ref.squeeze().unsqueeze(0))\n",
    "    # out = model.decoder(asr, F0_real, N_real, ref.squeeze().unsqueeze(0))\n",
    "\n",
    "wav = out.squeeze().cpu().numpy()[..., :-50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "sf.write(f'Demo/thalia-althaf-ts2s-{str(alpha).replace(\".\", \"_\")}-{str(beta).replace(\".\", \"_\")}.wav', wav, 24000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text, ref_s, alpha=0.3, beta=0.7, diffusion_steps=5, embedding_scale=1):\n",
    "    text = text.strip()\n",
    "    ps = global_phonemizer.phonemize([text])\n",
    "    ps = word_tokenize(ps[0])\n",
    "    ps = \" \".join(ps)\n",
    "    tokens = textclenaer(ps)\n",
    "    tokens.insert(0, 0)\n",
    "    tokens = torch.LongTensor(tokens).to(device).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_lengths = torch.LongTensor([tokens.shape[-1]]).to(device)\n",
    "        text_mask = length_to_mask(input_lengths).to(device)\n",
    "\n",
    "        t_en = model.text_encoder(tokens, input_lengths, text_mask)\n",
    "        bert_dur = model.bert(tokens, attention_mask=(~text_mask).int())\n",
    "        d_en = model.bert_encoder(bert_dur).transpose(-1, -2)\n",
    "\n",
    "        s_pred = sampler(\n",
    "            noise=torch.randn((1, 256)).unsqueeze(1).to(device),\n",
    "            embedding=bert_dur,\n",
    "            embedding_scale=embedding_scale,\n",
    "            features=ref_s,  # reference from the same speaker as the embedding\n",
    "            num_steps=diffusion_steps,\n",
    "        ).squeeze(1)\n",
    "\n",
    "        s = s_pred[:, 128:]\n",
    "        ref = s_pred[:, :128]\n",
    "\n",
    "        ref = alpha * ref + (1 - alpha) * ref_s[:, :128]\n",
    "        s = beta * s + (1 - beta) * ref_s[:, 128:]\n",
    "\n",
    "        d = model.predictor.text_encoder(d_en, s, input_lengths, text_mask)\n",
    "\n",
    "        x, _ = model.predictor.lstm(d)\n",
    "        duration = model.predictor.duration_proj(x)\n",
    "\n",
    "        duration = torch.sigmoid(duration).sum(axis=-1)\n",
    "        pred_dur = torch.round(duration.squeeze()).clamp(min=1)\n",
    "\n",
    "        pred_aln_trg = torch.zeros(input_lengths, int(pred_dur.sum().data))\n",
    "        c_frame = 0\n",
    "        for i in range(pred_aln_trg.size(0)):\n",
    "            pred_aln_trg[i, c_frame : c_frame + int(pred_dur[i].data)] = 1\n",
    "            c_frame += int(pred_dur[i].data)\n",
    "\n",
    "        # encode prosody\n",
    "        en = d.transpose(-1, -2) @ pred_aln_trg.unsqueeze(0).to(device)\n",
    "        if model_params.decoder.type == \"hifigan\":\n",
    "            asr_new = torch.zeros_like(en)\n",
    "            asr_new[:, :, 0] = en[:, :, 0]\n",
    "            asr_new[:, :, 1:] = en[:, :, 0:-1]\n",
    "            en = asr_new\n",
    "\n",
    "        F0_pred, N_pred = model.predictor.F0Ntrain(en, s)\n",
    "\n",
    "        asr = t_en @ pred_aln_trg.unsqueeze(0).to(device)\n",
    "        if model_params.decoder.type == \"hifigan\":\n",
    "            asr_new = torch.zeros_like(asr)\n",
    "            asr_new[:, :, 0] = asr[:, :, 0]\n",
    "            asr_new[:, :, 1:] = asr[:, :, 0:-1]\n",
    "            asr = asr_new\n",
    "\n",
    "        out = model.decoder(asr, F0_pred, N_pred, ref.squeeze().unsqueeze(0))\n",
    "\n",
    "    return (\n",
    "        out.squeeze().cpu().numpy()[..., :-50]\n",
    "    )  # weird pulse at the end of the model, need to be fixed later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthesize Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = \"\"\" StyleTTS 2 is a text to speech model that leverages style diffusion and adversarial training with large speech language models to achieve human level text to speech synthesis. \"\"\"\n",
    "# text = \"\"\" StyleTTS 2 adalah model text to speech yang memanfaatkan style diffusion dan pelatihan adversarial dengan model bahasa ucapan besar untuk mencapai sintesis text to speech level manusia. \"\"\"\n",
    "# text = \"\"\" Halo nama saya Budi dari Jakarta dan saya suka sekali membaca buku tentang sejarah Indonesia. \"\"\"\n",
    "text = \"\"\" Habari jina langu ni Victoria kutoka Tanzania, na napenda kusoma vitabu vya historia. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import soundfile as sf\n",
    "\n",
    "\n",
    "def synthesize_speech(reference_dicts):\n",
    "    start = time.time()\n",
    "    noise = torch.randn(1, 1, 256).to(device)\n",
    "    for k, path in reference_dicts.items():\n",
    "        try:\n",
    "            # Convert string path to Path object for easier manipulation\n",
    "            ref_s = compute_style(path)\n",
    "            path = Path(path)\n",
    "            # Create the output directory based on the reference path\n",
    "            output_directory = path.parent / \"synthesized_multilingual_en_id_sw\"\n",
    "            os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "            wav = inference(\n",
    "                text, ref_s, alpha=0.3, beta=0.1, diffusion_steps=10, embedding_scale=1\n",
    "            )\n",
    "\n",
    "            rtf = (time.time() - start) / (len(wav) / 24000)\n",
    "            print(f\"RTF = {rtf:5f}\")\n",
    "            import IPython.display as ipd\n",
    "\n",
    "            # print(k + \" Synthesized:\")\n",
    "            # display(ipd.Audio(wav, rate=24000, normalize=False))\n",
    "\n",
    "            sf.write(f\"{output_directory}/{path.name}\", wav, 24000)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "        # print(\"Reference:\")\n",
    "        # display(ipd.Audio(path, rate=24000, normalize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### US MADISON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_dicts = {}\n",
    "\n",
    "dir_path = Path(\"Demo/en-US-Madison\")\n",
    "\n",
    "# Iterate through each file in the directory\n",
    "for k, file_path in enumerate(sorted(dir_path.iterdir())):\n",
    "    if file_path.is_file():\n",
    "        reference_dicts[k] = file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesize_speech(reference_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UK THALIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_dicts = {}\n",
    "\n",
    "dir_path = Path(\"Demo/en-UK-Thalia\")\n",
    "\n",
    "# Iterate through each file in the directory\n",
    "for k, file_path in enumerate(sorted(dir_path.iterdir())):\n",
    "    if file_path.is_file():\n",
    "        reference_dicts[k] = file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesize_speech(reference_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AU Zak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_dicts = {}\n",
    "\n",
    "dir_path = Path(\"Demo/en-AU-Zak\")\n",
    "\n",
    "# Iterate through each file in the directory\n",
    "for k, file_path in enumerate(sorted(dir_path.iterdir())):\n",
    "    if file_path.is_file():\n",
    "        reference_dicts[k] = file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesize_speech(reference_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID ALTHAF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_dicts = {}\n",
    "\n",
    "dir_path = Path(\"Demo/id-ID-Althaf\")\n",
    "\n",
    "# Iterate through each file in the directory\n",
    "for k, file_path in enumerate(sorted(dir_path.iterdir())):\n",
    "    if file_path.is_file():\n",
    "        reference_dicts[k] = file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesize_speech(reference_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EN Althaf S2S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_dicts = {}\n",
    "\n",
    "dir_path = Path(\"Demo/en-Althaf-S2S\")\n",
    "\n",
    "# Iterate through each file in the directory\n",
    "for k, file_path in enumerate(sorted(dir_path.iterdir())):\n",
    "    if file_path.is_file():\n",
    "        reference_dicts[k] = file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesize_speech(reference_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SW Victoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_dicts = {}\n",
    "\n",
    "dir_path = Path(\"Demo/sw-TZ-Victoria\")\n",
    "\n",
    "# Iterate through each file in the directory\n",
    "for k, file_path in enumerate(sorted(dir_path.iterdir())):\n",
    "    if file_path.is_file():\n",
    "        reference_dicts[k] = file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RTF = 0.018854\n",
      "RTF = 0.040019\n",
      "RTF = 0.056657\n",
      "RTF = 0.071426\n",
      "RTF = 0.081993\n",
      "RTF = 0.107526\n"
     ]
    }
   ],
   "source": [
    "synthesize_speech(reference_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
