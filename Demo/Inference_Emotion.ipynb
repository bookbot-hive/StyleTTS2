{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/StyleTTS2\n",
      "177\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "%cd ..\n",
    "\n",
    "# load packages\n",
    "import time\n",
    "import random\n",
    "import yaml\n",
    "from munch import Munch\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import librosa\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer \n",
    "\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "\n",
    "from models import *\n",
    "from utils import *\n",
    "from text_utils import TextCleaner\n",
    "textclenaer = TextCleaner()\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_mel = torchaudio.transforms.MelSpectrogram(\n",
    "    n_mels=80, n_fft=2048, win_length=1200, hop_length=300\n",
    ")\n",
    "mean, std = -4, 4\n",
    "\n",
    "\n",
    "def length_to_mask(lengths):\n",
    "    mask = (\n",
    "        torch.arange(lengths.max())\n",
    "        .unsqueeze(0)\n",
    "        .expand(lengths.shape[0], -1)\n",
    "        .type_as(lengths)\n",
    "    )\n",
    "    mask = torch.gt(mask + 1, lengths.unsqueeze(1))\n",
    "    return mask\n",
    "\n",
    "\n",
    "def preprocess(wave):\n",
    "    wave_tensor = torch.from_numpy(wave).float()\n",
    "    mel_tensor = to_mel(wave_tensor)\n",
    "    mel_tensor = (torch.log(1e-5 + mel_tensor.unsqueeze(0)) - mean) / std\n",
    "    return mel_tensor\n",
    "\n",
    "\n",
    "def compute_style(path):\n",
    "    wave, sr = librosa.load(path, sr=24000)\n",
    "    audio, index = librosa.effects.trim(wave, top_db=30)\n",
    "    if sr != 24000:\n",
    "        audio = librosa.resample(audio, sr, 24000)\n",
    "    mel_tensor = preprocess(audio).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        ref_s = model.style_encoder(mel_tensor.unsqueeze(1))\n",
    "        ref_p = model.predictor_encoder(mel_tensor.unsqueeze(1))\n",
    "\n",
    "    return torch.cat([ref_s, ref_p], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/utils/weight_norm.py:28: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n",
      "  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n",
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# load phonemizer\n",
    "import phonemizer\n",
    "\n",
    "global_phonemizer = phonemizer.backend.EspeakBackend(\n",
    "    language=\"en-us\", preserve_punctuation=True, with_stress=True\n",
    ")\n",
    "\n",
    "config = yaml.safe_load(open(\"/root/StyleTTS2-en-Multi-id-Althaf/config_ft_en_multi_id_althaf.yml\"))\n",
    "# config = yaml.safe_load(open(\"/root/StyleTTS2-en-US-Madison/config_ft_madison.yml\"))\n",
    "\n",
    "# load pretrained ASR model\n",
    "ASR_config = config.get(\"ASR_config\", False)\n",
    "ASR_path = config.get(\"ASR_path\", False)\n",
    "text_aligner = load_ASR_models(ASR_path, ASR_config)\n",
    "\n",
    "# load pretrained F0 model\n",
    "F0_path = config.get(\"F0_path\", False)\n",
    "pitch_extractor = load_F0_models(F0_path)\n",
    "\n",
    "# load BERT model\n",
    "from Utils.PLBERT.util import load_plbert\n",
    "\n",
    "BERT_path = config.get(\"PLBERT_dir\", False)\n",
    "plbert = load_plbert(BERT_path)\n",
    "\n",
    "model_params = recursive_munch(config[\"model_params\"])\n",
    "model = build_model(model_params, text_aligner, pitch_extractor, plbert)\n",
    "_ = [model[key].eval() for key in model]\n",
    "_ = [model[key].to(device) for key in model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_whole = torch.load(\"/root/StyleTTS2-en-Multi-id-Althaf/epoch_2nd_00024.pth\", map_location=\"cpu\")\n",
    "# params_whole = torch.load(\"/root/StyleTTS2-en-US-Madison/epoch_2nd_00099.pth\", map_location=\"cpu\")\n",
    "params = params_whole[\"net\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert loaded\n",
      "bert_encoder loaded\n",
      "predictor loaded\n",
      "decoder loaded\n",
      "text_encoder loaded\n",
      "predictor_encoder loaded\n",
      "style_encoder loaded\n",
      "diffusion loaded\n",
      "text_aligner loaded\n",
      "pitch_extractor loaded\n",
      "mpd loaded\n",
      "msd loaded\n",
      "wd loaded\n"
     ]
    }
   ],
   "source": [
    "for key in model:\n",
    "    if key in params:\n",
    "        print(\"%s loaded\" % key)\n",
    "        try:\n",
    "            model[key].load_state_dict(params[key])\n",
    "        except:\n",
    "            from collections import OrderedDict\n",
    "\n",
    "            state_dict = params[key]\n",
    "            new_state_dict = OrderedDict()\n",
    "            for k, v in state_dict.items():\n",
    "                name = k[7:]  # remove `module.`\n",
    "                new_state_dict[name] = v\n",
    "            # load params\n",
    "            model[key].load_state_dict(new_state_dict, strict=False)\n",
    "#             except:\n",
    "#                 _load(params[key], model[key])\n",
    "_ = [model[key].eval() for key in model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Modules.diffusion.sampler import DiffusionSampler, ADPM2Sampler, KarrasSchedule\n",
    "\n",
    "sampler = DiffusionSampler(\n",
    "    model.diffusion.diffusion,\n",
    "    sampler=ADPM2Sampler(),\n",
    "    sigma_schedule=KarrasSchedule(\n",
    "        sigma_min=0.0001, sigma_max=3.0, rho=9.0\n",
    "    ),  # empirical parameters\n",
    "    clamp=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = TweetTokenizer() \n",
    "def inference(text, ref_s, alpha=0.3, beta=0.7, diffusion_steps=5, embedding_scale=1):\n",
    "    text = text.strip()\n",
    "    ps = global_phonemizer.phonemize([text])\n",
    "    ps = tk.tokenize(ps[0])\n",
    "    ps = \" \".join(ps)\n",
    "    tokens = textclenaer(ps)\n",
    "    tokens.insert(0, 0)\n",
    "    tokens = torch.LongTensor(tokens).to(device).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_lengths = torch.LongTensor([tokens.shape[-1]]).to(device)\n",
    "        text_mask = length_to_mask(input_lengths).to(device)\n",
    "\n",
    "        t_en = model.text_encoder(tokens, input_lengths, text_mask)\n",
    "        bert_dur = model.bert(tokens, attention_mask=(~text_mask).int())\n",
    "        d_en = model.bert_encoder(bert_dur).transpose(-1, -2)\n",
    "\n",
    "        s_pred = sampler(\n",
    "            noise=torch.randn((1, 256)).unsqueeze(1).to(device),\n",
    "            embedding=bert_dur,\n",
    "            embedding_scale=embedding_scale,\n",
    "            features=ref_s,\n",
    "            num_steps=diffusion_steps,\n",
    "        ).squeeze(1)\n",
    "\n",
    "        s = s_pred[:, 128:]\n",
    "        ref = s_pred[:, :128]\n",
    "\n",
    "        ref = alpha * ref + (1 - alpha) * ref_s[:, :128]\n",
    "        s = beta * s + (1 - beta) * ref_s[:, 128:]\n",
    "\n",
    "        d = model.predictor.text_encoder(d_en, s, input_lengths, text_mask)\n",
    "\n",
    "        x, _ = model.predictor.lstm(d)\n",
    "        duration = model.predictor.duration_proj(x)\n",
    "\n",
    "        duration = torch.sigmoid(duration).sum(axis=-1)\n",
    "        pred_dur = torch.round(duration.squeeze()).clamp(min=1)\n",
    "\n",
    "        pred_aln_trg = torch.zeros(input_lengths, int(pred_dur.sum().data))\n",
    "        c_frame = 0\n",
    "        for i in range(pred_aln_trg.size(0)):\n",
    "            pred_aln_trg[i, c_frame : c_frame + int(pred_dur[i].data)] = 1\n",
    "            c_frame += int(pred_dur[i].data)\n",
    "\n",
    "        # encode prosody\n",
    "        en = d.transpose(-1, -2) @ pred_aln_trg.unsqueeze(0).to(device)\n",
    "        if model_params.decoder.type == \"hifigan\":\n",
    "            asr_new = torch.zeros_like(en)\n",
    "            asr_new[:, :, 0] = en[:, :, 0]\n",
    "            asr_new[:, :, 1:] = en[:, :, 0:-1]\n",
    "            en = asr_new\n",
    "\n",
    "        F0_pred, N_pred = model.predictor.F0Ntrain(en, s)\n",
    "\n",
    "        asr = t_en @ pred_aln_trg.unsqueeze(0).to(device)\n",
    "        if model_params.decoder.type == \"hifigan\":\n",
    "            asr_new = torch.zeros_like(asr)\n",
    "            asr_new[:, :, 0] = asr[:, :, 0]\n",
    "            asr_new[:, :, 1:] = asr[:, :, 0:-1]\n",
    "            asr = asr_new\n",
    "\n",
    "        out = model.decoder(asr, F0_pred, N_pred, ref.squeeze().unsqueeze(0))\n",
    "\n",
    "    return (\n",
    "        out.squeeze().cpu().numpy()[..., :-50]\n",
    "    )  # weird pulse at the end of the model, need to be fixed later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthesize Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import soundfile as sf\n",
    "\n",
    "\n",
    "def synthesize_speech(reference_dicts):\n",
    "    start = time.time()\n",
    "    noise = torch.randn(1, 1, 256).to(device)\n",
    "    for path, text in reference_dicts.items():\n",
    "        try:\n",
    "            # Convert string path to Path object for easier manipulation\n",
    "            ref_s = compute_style(path)\n",
    "            path = Path(path)\n",
    "            # Create the output directory based on the reference path\n",
    "            # output_directory = path.parent / \"synthesized_en_madison\"\n",
    "            output_directory = path.parent / \"synthesized_multilingual_en_id\"\n",
    "            os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "            wav = inference(\n",
    "                text, ref_s, alpha=0.0, beta=0.0, diffusion_steps=8, embedding_scale=1\n",
    "            )\n",
    "\n",
    "            rtf = (time.time() - start) / (len(wav) / 24000)\n",
    "            print(f\"RTF = {rtf:5f}\")\n",
    "            import IPython.display as ipd\n",
    "\n",
    "            # print(k + \" Synthesized:\")\n",
    "            # display(ipd.Audio(wav, rate=24000, normalize=False))\n",
    "            sf.write(f\"{output_directory}/{path.name}\", wav, 24000)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "        # print(\"Reference:\")\n",
    "        # display(ipd.Audio(path, rate=24000, normalize=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### US MADISON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Angry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_dicts = {}\n",
    "        \n",
    "# reference_dicts = {\n",
    "#     \"/root/StyleTTS2/Demo/US-Madison/Happy.wav\": \"\"\"“ I’m absolutely over the moon right now! This moment has filled me with overwhelming joy and gratitude!\" said he happily. \"\"\",\n",
    "#     \"/root/StyleTTS2/Demo/US-Madison/Happy_concatenated_audio.wav\": \"\"\" The joyous laughter of children echoed through the park, filling the air with a symphony of bliss! said he joyfully \"\"\",\n",
    "#     \"/root/StyleTTS2/Demo/US-Madison/Angry.wav\": \"\"\"\" The field of astronomy is a joke! Its theories are based on flawed observations!\" she said, angrily. \"\"\",\n",
    "#     \"/root/StyleTTS2/Demo/US-Madison/Sad.wav\": \"\"\" \"My heart is heavy with profound sadness. This moment has left me feeling deeply sorrowful and lost.\" said he in a sad tone. \"\"\",\n",
    "#     \"/root/StyleTTS2/Demo/US-Madison/Whisper.wav\": \"\"\" \"Did you hear what happened? Keep this a secret, okay.” he whispered softly. \"\"\",\n",
    "#     \"/root/StyleTTS2/Demo/US-Madison/Shouting.wav\": \"\"\" “Watch out! There’s a car coming from the right side, and it’s moving fast!” she shouted. \"\"\",\n",
    "#     \"/root/StyleTTS2/Demo/US-Madison/Shouting Angry.wav\": \"\"\" The field of astronomy is a joke! Its theories are based on flawed observations!\" she said, angrily. \"\"\",\n",
    "# }\n",
    "\n",
    "reference_dicts = {\n",
    "    # \"/root/StyleTTS2/Demo/US-Madison/Happy.wav\": \"\"\" The joyous laughter of children echoed through the park! filling the air with a symphony of bliss! \"\"\",\n",
    "    # \"/root/StyleTTS2/Demo/US-Madison/Happy_concatenated_audio.wav\": \"\"\" The joyous laughter of children echoed through the park! filling the air with a symphony of bliss! \"\"\",\n",
    "    # \"/root/StyleTTS2/Demo/US-Madison/Angry.wav\": \"\"\" I can’t believe this happened! This is absolutely infuriating! \"\"\",\n",
    "    # \"/root/StyleTTS2/Demo/US-Madison/Angry_concatenated_audio.wav\": \"\"\" I can’t believe this happened! This is absolutely infuriating! \"\"\",\n",
    "    # \"/root/StyleTTS2/Demo/US-Madison/Sad.wav\": \"\"\" I feel utterly heartbroken. This news has left me devastated. \"\"\",\n",
    "    \"/root/StyleTTS2/Demo/US-Madison/Adrian_Whisper_cleaned.wav\": \"\"\" Did you hear that? I think I heard someone's coming. \"\"\",\n",
    "    \"/root/StyleTTS2/Demo/US-Madison/Madison_whisper.wav\": \"\"\" Did you hear that? I think I heard someone's coming. \"\"\",\n",
    "    \"/root/StyleTTS2/Demo/US-Madison/David_Eleven_Labs_Whisper.wav\": \"\"\" Did you hear that? I think I heard someone's coming. \"\"\",\n",
    "    \"/root/StyleTTS2/Demo/US-Madison/Whisper_adrian_concat.mp3\": \"\"\" Don’t let anyone else know about this. Meet me quietly in the hallway. \"\"\",\n",
    "    \"/root/StyleTTS2/Demo/US-Madison/Whisper_adrian_concat_2.mp3\": \"\"\" Don’t let anyone else know about this. Meet me quietly in the hallway. \"\"\",\n",
    "    \"/root/StyleTTS2/Demo/US-Madison/Whisper_concatenated_audio.wav\": \"\"\" Don’t let anyone else know about this. Meet me quietly in the hallway. \"\"\",\n",
    "    # \"/root/StyleTTS2/Demo/US-Madison/Shouting.wav\": \"\"\" Hey! Look out for the incoming tiger! \"\"\",\n",
    "    # \"/root/StyleTTS2/Demo/US-Madison/Shouting_concatenated_audio.wav\": \"\"\" Hey! Look out for the incoming tiger! \"\"\",\n",
    "    # \"/root/StyleTTS2/Demo/US-Madison/Shouting-Angry-1.wav\": \"\"\" I am so angry right now! I don't want to see him anymore! \"\"\",\n",
    "    # \"/root/StyleTTS2/Demo/US-Madison/Shouting-Angry_concatenated_audio.wav\": \"\"\" I am so angry right now! I don't want to see him anymore! \"\"\",\n",
    "}\n",
    "\n",
    "# reference_dicts = {\n",
    "#     \"/root/StyleTTS2/Demo/US-Madison/Happy.wav\": \"\"\" I just got amazing news! Today is such a joyful day! \"\"\",\n",
    "#     \"/root/StyleTTS2/Demo/US-Madison/Angry.wav\": \"\"\" I can’t believe this happened! This is absolutely infuriating! \"\"\",\n",
    "#     \"/root/StyleTTS2/Demo/US-Madison/Shouting Angry.wav\": \"\"\" I can’t believe this happened! This is absolutely infuriating! \"\"\",\n",
    "#     \"/root/StyleTTS2/Demo/US-Madison/Sad.wav\": \"\"\" I feel utterly heartbroken. This news has left me devastated. \"\"\",\n",
    "#     \"/root/StyleTTS2/Demo/US-Madison/Whisper.wav\": \"\"\" Don’t let anyone else know about this. Meet me quietly in the hallway. \"\"\",\n",
    "#     \"/root/StyleTTS2/Demo/US-Madison/Shouting.wav\": \"\"\" Get out of the way right now! Everyone, listen up, we need to move quickly! \"\"\",\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RTF = 0.042922\n",
      "RTF = 0.082087\n",
      "RTF = 0.174904\n",
      "RTF = 0.122203\n",
      "RTF = 0.168355\n",
      "RTF = 0.228435\n"
     ]
    }
   ],
   "source": [
    "synthesize_speech(reference_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpeechToSpeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = TweetTokenizer() \n",
    "def s2s(text, ref_s, target_s, alpha=0.8, beta=0.1, diffusion_steps=10, embedding_scale=1):\n",
    "    text = text.strip()\n",
    "    ps = global_phonemizer.phonemize([text])\n",
    "    ps = tk.tokenize(ps[0])\n",
    "    ps = \" \".join(ps)\n",
    "    tokens = textclenaer(ps)\n",
    "    tokens.insert(0, 0)\n",
    "    tokens = torch.LongTensor(tokens).to(device).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_lengths = torch.LongTensor([tokens.shape[-1]]).to(device)\n",
    "        text_mask = length_to_mask(input_lengths).to(device)\n",
    "\n",
    "        t_en = model.text_encoder(tokens, input_lengths, text_mask)\n",
    "        bert_dur = model.bert(tokens, attention_mask=(~text_mask).int())\n",
    "        d_en = model.bert_encoder(bert_dur).transpose(-1, -2)\n",
    "\n",
    "        s_pred = sampler(\n",
    "            noise=torch.randn((1, 256)).unsqueeze(1).to(device),\n",
    "            embedding=bert_dur,\n",
    "            embedding_scale=embedding_scale,\n",
    "            features=target_s,  # reference from the same speaker as the embedding\n",
    "            num_steps=diffusion_steps,\n",
    "        ).squeeze(1)\n",
    "\n",
    "        ref = s_pred[:, :128]\n",
    "        s = s_pred[:, 128:]\n",
    "\n",
    "        ref = alpha * ref + (1 - alpha) * ref_s[:, :128]\n",
    "        s = beta * s + (1 - beta) * ref_s[:, 128:]\n",
    "\n",
    "        d = model.predictor.text_encoder(d_en, s, input_lengths, text_mask)\n",
    "\n",
    "        x, _ = model.predictor.lstm(d)\n",
    "        duration = model.predictor.duration_proj(x)\n",
    "\n",
    "        duration = torch.sigmoid(duration).sum(axis=-1)\n",
    "        pred_dur = torch.round(duration.squeeze()).clamp(min=1)\n",
    "\n",
    "        pred_aln_trg = torch.zeros(input_lengths, int(pred_dur.sum().data))\n",
    "        c_frame = 0\n",
    "        for i in range(pred_aln_trg.size(0)):\n",
    "            pred_aln_trg[i, c_frame : c_frame + int(pred_dur[i].data)] = 1\n",
    "            c_frame += int(pred_dur[i].data)\n",
    "\n",
    "        # encode prosody\n",
    "        en = d.transpose(-1, -2) @ pred_aln_trg.unsqueeze(0).to(device)\n",
    "        if model_params.decoder.type == \"hifigan\":\n",
    "            asr_new = torch.zeros_like(en)\n",
    "            asr_new[:, :, 0] = en[:, :, 0]\n",
    "            asr_new[:, :, 1:] = en[:, :, 0:-1]\n",
    "            en = asr_new\n",
    "\n",
    "        F0_pred, N_pred = model.predictor.F0Ntrain(en, s)\n",
    "\n",
    "        asr = t_en @ pred_aln_trg.unsqueeze(0).to(device)\n",
    "        if model_params.decoder.type == \"hifigan\":\n",
    "            asr_new = torch.zeros_like(asr)\n",
    "            asr_new[:, :, 0] = asr[:, :, 0]\n",
    "            asr_new[:, :, 1:] = asr[:, :, 0:-1]\n",
    "            asr = asr_new\n",
    "\n",
    "        out = model.decoder(asr, F0_pred, N_pred, ref.squeeze().unsqueeze(0))\n",
    "\n",
    "    return out.squeeze().cpu().numpy()[..., :-50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Did you hear that? I think I heard someone's coming.\"\n",
    "# text = \"Did you hear that? I think I heard something. Can you keep a secret? I think someone's coming, so be quiet. We have to be very careful from here on out. I need to tell you something important.\"\n",
    "ref_david = compute_style(\"/root/StyleTTS2/Demo/US-Madison/David_Eleven_Labs_Whisper_Short.wav\")\n",
    "ref_madison = compute_style(\"/root/StyleTTS2/Demo/US-Madison/madison_concatenated_audio.wav\")\n",
    "\n",
    "wav = s2s(text, ref_david, ref_madison, alpha=0.7, beta=0.9, diffusion_steps=10)\n",
    "sf.write(f'/root/StyleTTS2/Demo/US-Madison/david-madison-s2s.wav', wav, 24000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UK THALIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Demo/en-UK-Thalia'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m dir_path \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDemo/en-UK-Thalia\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Iterate through each file in the directory\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, file_path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdir_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_path\u001b[38;5;241m.\u001b[39mis_file():\n\u001b[1;32m      8\u001b[0m         reference_dicts[k] \u001b[38;5;241m=\u001b[39m file_path\n",
      "File \u001b[0;32m/usr/lib/python3.10/pathlib.py:1017\u001b[0m, in \u001b[0;36mPath.iterdir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1013\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21miterdir\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Iterate over the files in this directory.  Does not yield any\u001b[39;00m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;124;03m    result for the special paths '.' and '..'.\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1017\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_accessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1018\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m}:\n\u001b[1;32m   1019\u001b[0m             \u001b[38;5;66;03m# Yielding a path object for these makes little sense\u001b[39;00m\n\u001b[1;32m   1020\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Demo/en-UK-Thalia'"
     ]
    }
   ],
   "source": [
    "reference_dicts = {}\n",
    "\n",
    "dir_path = Path(\"Demo/en-UK-Thalia\")\n",
    "\n",
    "# Iterate through each file in the directory\n",
    "for k, file_path in enumerate(sorted(dir_path.iterdir())):\n",
    "    if file_path.is_file():\n",
    "        reference_dicts[k] = file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesize_speech(reference_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AU Zak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_dicts = {}\n",
    "\n",
    "dir_path = Path(\"Demo/en-AU-Zak\")\n",
    "\n",
    "# Iterate through each file in the directory\n",
    "for k, file_path in enumerate(sorted(dir_path.iterdir())):\n",
    "    if file_path.is_file():\n",
    "        reference_dicts[k] = file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesize_speech(reference_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ID ALTHAF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_dicts = {}\n",
    "\n",
    "dir_path = Path(\"Demo/id-ID-Althaf\")\n",
    "\n",
    "# Iterate through each file in the directory\n",
    "for k, file_path in enumerate(sorted(dir_path.iterdir())):\n",
    "    if file_path.is_file():\n",
    "        reference_dicts[k] = file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesize_speech(reference_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EN Althaf S2S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_dicts = {}\n",
    "\n",
    "dir_path = Path(\"Demo/en-Althaf-S2S\")\n",
    "\n",
    "# Iterate through each file in the directory\n",
    "for k, file_path in enumerate(sorted(dir_path.iterdir())):\n",
    "    if file_path.is_file():\n",
    "        reference_dicts[k] = file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesize_speech(reference_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SW Victoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_dicts = {}\n",
    "\n",
    "dir_path = Path(\"Demo/sw-TZ-Victoria\")\n",
    "\n",
    "# Iterate through each file in the directory\n",
    "for k, file_path in enumerate(sorted(dir_path.iterdir())):\n",
    "    if file_path.is_file():\n",
    "        reference_dicts[k] = file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesize_speech(reference_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
