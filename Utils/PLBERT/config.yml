log_dir: "Checkpoint"
mixed_precision: "bf16"
data_folder: "bookbot/wikipedia_id_phonemized"
batch_size: 32
save_interval: 100000
log_interval: 100
num_process: 1 # number of GPUs
num_steps: 1000000

dataset_params:
  tokenizer: indobenchmark/indobert-base-p1
  token_separator: " " # token used for phoneme separator
  token_mask: "[MASK]" # token used for phoneme mask
  word_separator: "[SEP]" # token used for word separator

  max_mel_length: 512 # max phoneme length

  word_mask_prob: 0.15 # probability to mask the entire word
  phoneme_mask_prob: 0.1 # probability to mask each phoneme
  replace_prob: 0.2 # probablity to replace phonemes

model_params:
  vocab_size: 38
  hidden_size: 768
  num_attention_heads: 12
  intermediate_size: 2048
  max_position_embeddings: 512
  num_hidden_layers: 12
  dropout: 0.1
